# æ— äººæœºè§†è§‰MLflowå®éªŒ - PyCharmä½¿ç”¨è¯´æ˜

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ— äººæœºè§†è§‰æœºå™¨å­¦ä¹ å®éªŒå¹³å°ï¼Œé›†æˆäº†MLflowå®éªŒè·Ÿè¸ªã€æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒã€Web UIç•Œé¢å’Œå¯è§†åŒ–åŠŸèƒ½ã€‚é¡¹ç›®é€‚ç”¨äºæ— äººæœºè§†è§‰ç®—æ³•å¼€å‘ä¸è®­ç»ƒå®ä¹ ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

#### 1.1 å®‰è£…Pythonç¯å¢ƒ
- ç¡®ä¿å·²å®‰è£…Python 3.8æˆ–æ›´é«˜ç‰ˆæœ¬
- æ¨èä½¿ç”¨Anacondaæˆ–Minicondaç®¡ç†Pythonç¯å¢ƒ

#### 1.2 åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
```bash
# ä½¿ç”¨condaåˆ›å»ºç¯å¢ƒ
conda create -n drone_vision python=3.9
conda activate drone_vision

# æˆ–ä½¿ç”¨venvåˆ›å»ºç¯å¢ƒ
python -m venv drone_vision_env
# Windowsæ¿€æ´»
drone_vision_env\Scripts\activate
# Linux/Macæ¿€æ´»
source drone_vision_env/bin/activate
```

### 2. PyCharmé¡¹ç›®é…ç½®

#### 2.1 æ‰“å¼€é¡¹ç›®
1. å¯åŠ¨PyCharm
2. é€‰æ‹© `File` â†’ `Open`
3. é€‰æ‹©é¡¹ç›®æ ¹ç›®å½• `D:\mlflow_learning_project`
4. ç‚¹å‡» `OK`

#### 2.2 é…ç½®Pythonè§£é‡Šå™¨
1. æ‰“å¼€ `File` â†’ `Settings` (æˆ– `Ctrl+Alt+S`)
2. å¯¼èˆªåˆ° `Project: mlflow_learning_project` â†’ `Python Interpreter`
3. ç‚¹å‡»é½¿è½®å›¾æ ‡ â†’ `Add`
4. é€‰æ‹© `Conda Environment` â†’ `Existing environment`
5. é€‰æ‹©ä¹‹å‰åˆ›å»ºçš„ `drone_vision` ç¯å¢ƒ
6. ç‚¹å‡» `OK`

#### 2.3 å®‰è£…ä¾èµ–åŒ…
åœ¨PyCharmç»ˆç«¯ä¸­è¿è¡Œï¼š
```bash
pip install -r requirements.txt
```

### 3. é¡¹ç›®ç»“æ„è¯´æ˜

```
mlflow_learning_project/
â”œâ”€â”€ main.py                 # ä¸»å®éªŒç¨‹åº
â”œâ”€â”€ streamlit_app.py        # Web UIç•Œé¢
â”œâ”€â”€ requirements.txt        # ä¾èµ–åŒ…åˆ—è¡¨
â”œâ”€â”€ PyCharmä½¿ç”¨è¯´æ˜.md      # æœ¬è¯´æ˜æ–‡æ¡£
â””â”€â”€ README.md              # é¡¹ç›®è¯´æ˜
```

## ğŸ”§ è¯¦ç»†ä½¿ç”¨æ­¥éª¤

### æ­¥éª¤1: è¿è¡ŒåŸºç¡€å®éªŒ

#### 1.1 ç›´æ¥è¿è¡Œä¸»ç¨‹åº
1. åœ¨PyCharmä¸­æ‰“å¼€ `main.py`
2. å³é”®ç‚¹å‡»æ–‡ä»¶ â†’ `Run 'main'`
3. æˆ–åœ¨ç»ˆç«¯ä¸­è¿è¡Œï¼š
```bash
python main.py
```

#### 1.2 æŸ¥çœ‹å®éªŒç»“æœ
- å®éªŒå®Œæˆåä¼šæ˜¾ç¤ºè®­ç»ƒè¿‡ç¨‹
- ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ `drone_vision_analysis.png`
- MLflowå®éªŒè®°å½•è‡ªåŠ¨ä¿å­˜

### æ­¥éª¤2: å¯åŠ¨Web UIç•Œé¢

#### 2.1 å¯åŠ¨Streamlitåº”ç”¨
1. åœ¨PyCharmç»ˆç«¯ä¸­è¿è¡Œï¼š
```bash
streamlit run streamlit_app.py
```

2. æµè§ˆå™¨ä¼šè‡ªåŠ¨æ‰“å¼€ `http://localhost:8501`

#### 2.2 ä½¿ç”¨Webç•Œé¢åŠŸèƒ½
- **è¿è¡Œæ–°å®éªŒ**: è®¾ç½®å‚æ•°å¹¶å¼€å§‹è®­ç»ƒ
- **æŸ¥çœ‹å†å²å®éªŒ**: åˆ†æä¹‹å‰çš„å®éªŒç»“æœ
- **æ¨¡å‹å¯¹æ¯”åˆ†æ**: æ¯”è¾ƒä¸åŒæ¨¡å‹æ€§èƒ½

### æ­¥éª¤3: ä½¿ç”¨MLflow UI

#### 3.1 å¯åŠ¨MLflowæœåŠ¡å™¨
```bash
mlflow ui
```

#### 3.2 è®¿é—®MLflowç•Œé¢
- æ‰“å¼€æµè§ˆå™¨è®¿é—® `http://localhost:5000`
- æŸ¥çœ‹å®éªŒè®°å½•ã€å‚æ•°ã€æŒ‡æ ‡å’Œæ¨¡å‹

### æ­¥éª¤4: è‡ªå®šä¹‰å®éªŒ

#### 4.1 ä¿®æ”¹å®éªŒå‚æ•°
åœ¨ `main.py` ä¸­ä¿®æ”¹ä»¥ä¸‹å‚æ•°ï¼š
```python
# æ•°æ®å‚æ•°
num_samples = 1000  # æ ·æœ¬æ•°é‡
image_size = (64, 64)  # å›¾åƒå°ºå¯¸

# è®­ç»ƒå‚æ•°
num_epochs = 10  # è®­ç»ƒè½®æ•°
learning_rate = 0.001  # å­¦ä¹ ç‡
batch_size = 32  # æ‰¹æ¬¡å¤§å°
```

#### 4.2 ä¿®æ”¹æ¨¡å‹æ¶æ„
åœ¨ `DroneVisionCNN` ç±»ä¸­è‡ªå®šä¹‰ç½‘ç»œç»“æ„ï¼š
```python
class DroneVisionCNN(nn.Module):
    def __init__(self, num_classes=5):
        # ä¿®æ”¹ç½‘ç»œå±‚
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3)  # å¢åŠ é€šé“æ•°
        # æ·»åŠ æ›´å¤šå±‚...
```

#### 4.3 æ·»åŠ æ–°çš„æ•°æ®å¢å¼º
åœ¨ `prepare_data` æ–¹æ³•ä¸­ä¿®æ”¹æ•°æ®å˜æ¢ï¼š
```python
transform_train = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((64, 64)),
    transforms.RandomHorizontalFlip(0.5),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # æ·»åŠ é¢œè‰²å˜æ¢
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
```

## ğŸ› ï¸ é«˜çº§åŠŸèƒ½

### 1. å®éªŒè·Ÿè¸ªå’Œç‰ˆæœ¬æ§åˆ¶

#### 1.1 ä½¿ç”¨MLflowè®°å½•å®éªŒ
```python
with mlflow.start_run():
    # è®°å½•å‚æ•°
    mlflow.log_param("learning_rate", 0.001)
    mlflow.log_param("num_epochs", 10)
    
    # è®°å½•æŒ‡æ ‡
    mlflow.log_metric("accuracy", 0.85)
    mlflow.log_metric("loss", 0.15)
    
    # ä¿å­˜æ¨¡å‹
    mlflow.pytorch.log_model(model, "model")
```

#### 1.2 å®éªŒå¯¹æ¯”
åœ¨MLflow UIä¸­å¯ä»¥ï¼š
- æ¯”è¾ƒä¸åŒå®éªŒçš„å‚æ•°å’Œç»“æœ
- æŸ¥çœ‹è®­ç»ƒæ›²çº¿
- ä¸‹è½½æœ€ä½³æ¨¡å‹

### 2. æ¨¡å‹ä¼˜åŒ–

#### 2.1 è¶…å‚æ•°è°ƒä¼˜
```python
import optuna

def objective(trial):
    lr = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)
    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])
    
    # è®­ç»ƒæ¨¡å‹
    accuracy = train_model(lr, batch_size)
    return accuracy

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=20)
```

#### 2.2 æ¨¡å‹é›†æˆ
```python
# åˆ›å»ºå¤šä¸ªæ¨¡å‹
models = [DroneVisionCNN() for _ in range(5)]

# é›†æˆé¢„æµ‹
def ensemble_predict(models, x):
    predictions = []
    for model in models:
        pred = model(x)
        predictions.append(pred)
    return torch.mean(torch.stack(predictions), dim=0)
```

### 3. æ•°æ®ç®¡ç†

#### 3.1 çœŸå®æ•°æ®åŠ è½½
```python
def load_real_data(data_path):
    """åŠ è½½çœŸå®æ— äººæœºæ•°æ®"""
    images = []
    labels = []
    
    for class_name in os.listdir(data_path):
        class_path = os.path.join(data_path, class_name)
        for img_file in os.listdir(class_path):
            img_path = os.path.join(class_path, img_file)
            img = cv2.imread(img_path)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            images.append(img)
            labels.append(class_name)
    
    return images, labels
```

#### 3.2 æ•°æ®é¢„å¤„ç†ç®¡é“
```python
class DataPreprocessor:
    def __init__(self):
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
    
    def preprocess(self, image):
        return self.transform(image)
```

## ğŸ“Š å¯è§†åŒ–å’Œåˆ†æ

### 1. è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
```python
def plot_training_history(history):
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # æŸå¤±æ›²çº¿
    axes[0, 0].plot(history['train_losses'], label='è®­ç»ƒæŸå¤±')
    axes[0, 0].plot(history['val_losses'], label='éªŒè¯æŸå¤±')
    axes[0, 0].set_title('æ¨¡å‹æŸå¤±')
    axes[0, 0].legend()
    
    # å‡†ç¡®ç‡æ›²çº¿
    axes[0, 1].plot(history['train_accuracies'], label='è®­ç»ƒå‡†ç¡®ç‡')
    axes[0, 1].plot(history['val_accuracies'], label='éªŒè¯å‡†ç¡®ç‡')
    axes[0, 1].set_title('æ¨¡å‹å‡†ç¡®ç‡')
    axes[0, 1].legend()
    
    plt.tight_layout()
    plt.show()
```

### 2. æ¨¡å‹æ€§èƒ½åˆ†æ
```python
def analyze_model_performance(model, test_loader):
    model.eval()
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for data, target in test_loader:
            output = model(data)
            _, predicted = torch.max(output, 1)
            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
    
    # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
    report = classification_report(all_targets, all_predictions)
    print(report)
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(all_targets, all_predictions)
    sns.heatmap(cm, annot=True, fmt='d')
    plt.title('æ··æ·†çŸ©é˜µ')
    plt.show()
```

## ğŸ› å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### é—®é¢˜1: å†…å­˜ä¸è¶³
**è§£å†³æ–¹æ¡ˆ:**
```python
# å‡å°‘æ‰¹æ¬¡å¤§å°
batch_size = 16  # ä»32å‡å°‘åˆ°16

# ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
accumulation_steps = 2
for i, (data, target) in enumerate(train_loader):
    output = model(data)
    loss = criterion(output, target) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

### é—®é¢˜2: è®­ç»ƒé€Ÿåº¦æ…¢
**è§£å†³æ–¹æ¡ˆ:**
```python
# ä½¿ç”¨GPUåŠ é€Ÿ
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

with autocast():
    output = model(data)
    loss = criterion(output, target)
```

### é—®é¢˜3: è¿‡æ‹Ÿåˆ
**è§£å†³æ–¹æ¡ˆ:**
```python
# å¢åŠ æ­£åˆ™åŒ–
model = nn.Sequential(
    nn.Conv2d(3, 32, 3),
    nn.BatchNorm2d(32),
    nn.ReLU(),
    nn.Dropout(0.5),  # å¢åŠ Dropout
    # ... æ›´å¤šå±‚
)

# ä½¿ç”¨æ•°æ®å¢å¼º
transform_train = transforms.Compose([
    transforms.RandomHorizontalFlip(0.5),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.2),
    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
])
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ•°æ®åŠ è½½ä¼˜åŒ–
```python
# ä½¿ç”¨å¤šè¿›ç¨‹æ•°æ®åŠ è½½
train_loader = DataLoader(
    dataset, 
    batch_size=32, 
    shuffle=True, 
    num_workers=4,  # ä½¿ç”¨4ä¸ªè¿›ç¨‹
    pin_memory=True  # å›ºå®šå†…å­˜
)
```

### 2. æ¨¡å‹ä¼˜åŒ–
```python
# ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
import torchvision.models as models
model = models.resnet18(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, num_classes)

# å†»ç»“æ—©æœŸå±‚
for param in model.parameters():
    param.requires_grad = False
for param in model.fc.parameters():
    param.requires_grad = True
```

### 3. è®­ç»ƒç­–ç•¥
```python
# ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=5
)

# æ—©åœæœºåˆ¶
class EarlyStopping:
    def __init__(self, patience=7, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
    
    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
        else:
            self.counter += 1
        
        return self.counter >= self.patience
```

## ğŸ”— ç›¸å…³èµ„æº

### å­¦ä¹ èµ„æº
- [PyTorchå®˜æ–¹æ–‡æ¡£](https://pytorch.org/docs/)
- [MLflowå®˜æ–¹æ–‡æ¡£](https://mlflow.org/docs/)
- [Streamlitå®˜æ–¹æ–‡æ¡£](https://docs.streamlit.io/)

### æ•°æ®é›†èµ„æº
- [COCOæ•°æ®é›†](https://cocodataset.org/)
- [ImageNetæ•°æ®é›†](https://www.image-net.org/)
- [æ— äººæœºæ•°æ®é›†](https://www.kaggle.com/datasets)

### æ¨¡å‹èµ„æº
- [PyTorchæ¨¡å‹åº“](https://pytorch.org/vision/stable/models.html)
- [Hugging Faceæ¨¡å‹åº“](https://huggingface.co/models)

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœåœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œå¯ä»¥ï¼š

1. æŸ¥çœ‹PyCharmæ§åˆ¶å°è¾“å‡ºçš„é”™è¯¯ä¿¡æ¯
2. æ£€æŸ¥MLflow UIä¸­çš„å®éªŒè®°å½•
3. æŸ¥çœ‹ç”Ÿæˆçš„æ—¥å¿—æ–‡ä»¶
4. å‚è€ƒæœ¬æ–‡æ¡£çš„å¸¸è§é—®é¢˜éƒ¨åˆ†

## ğŸ¯ ä¸‹ä¸€æ­¥è®¡åˆ’

1. **æ•°æ®å¢å¼º**: å®ç°æ›´å¤šæ•°æ®å¢å¼ºæŠ€æœ¯
2. **æ¨¡å‹é›†æˆ**: å®ç°å¤šæ¨¡å‹é›†æˆ
3. **å®æ—¶æ¨ç†**: æ·»åŠ å®æ—¶å›¾åƒåˆ†ç±»åŠŸèƒ½
4. **æ¨¡å‹éƒ¨ç½²**: ä½¿ç”¨FastAPIéƒ¨ç½²æ¨¡å‹æœåŠ¡
5. **ç§»åŠ¨ç«¯åº”ç”¨**: å¼€å‘ç§»åŠ¨ç«¯æ— äººæœºè§†è§‰åº”ç”¨

---

**ç¥æ‚¨å®éªŒé¡ºåˆ©ï¼** ğŸšâœ¨
