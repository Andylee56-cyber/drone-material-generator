# æ— äººæœºè§†è§‰ç®—æ³•å¹³å°å®ä¹  - ç¬¬3-4å‘¨å·¥ä½œè¯¦ç»†æ–¹æ¡ˆ

## ğŸ“‹ å·¥ä½œæ¦‚è¿°

**é˜¶æ®µ**: ç¬¬ä¸€é˜¶æ®µ - æŠ€æœ¯åŸºç¡€å»ºè®¾ä¸å¹³å°æ­å»º  
**å‘¨æœŸ**: ç¬¬3-4å‘¨  
**ä¸»é¢˜**: æ•°æ®ç®¡ç†ç³»ç»Ÿå»ºè®¾  
**ç›®æ ‡**: å»ºç«‹å®Œæ•´çš„æ•°æ®ç‰ˆæœ¬ç®¡ç†ã€å­˜å‚¨æ¶æ„å’Œæ ‡æ³¨æµç¨‹

---

## ğŸ¯ ç¬¬3-4å‘¨æ ¸å¿ƒä»»åŠ¡

### ä»»åŠ¡1: éƒ¨ç½²DVC (Data Version Control) è¿›è¡Œæ•°æ®ç‰ˆæœ¬ç®¡ç†
### ä»»åŠ¡2: è®¾è®¡æ— äººæœºæ•°æ®çš„å­˜å‚¨æ¶æ„å’Œæ ‡æ³¨æµç¨‹
### ä»»åŠ¡3: å»ºç«‹æ•°æ®å®‰å…¨å’Œéšç§ä¿æŠ¤æœºåˆ¶
### ä»»åŠ¡4: æµ‹è¯•å°è§„æ¨¡æ•°æ®é›†çš„å®Œæ•´è®­ç»ƒæµç¨‹

---

## ğŸ“… è¯¦ç»†å·¥ä½œè®¡åˆ’

### ç¬¬3å‘¨ï¼šæ•°æ®ç‰ˆæœ¬ç®¡ç†ä¸å­˜å‚¨æ¶æ„

#### å‘¨ä¸€ï¼šDVCç¯å¢ƒæ­å»ºä¸é…ç½®

**ä¸Šåˆ (9:00-12:00) - DVCåŸºç¡€ç¯å¢ƒæ­å»º**

**9:00-9:30 ç†è®ºå­¦ä¹ **
- [ ] **å­¦ä¹ DVCåŸºæœ¬æ¦‚å¿µ**ï¼ˆ30åˆ†é’Ÿï¼‰
  - é˜…è¯»DVCå®˜æ–¹æ–‡æ¡£ï¼šhttps://dvc.org/doc/start
  - ç†è§£DVCä¸Gitçš„å…³ç³»ï¼šDVCæ˜¯æ•°æ®ç‰ˆæœ¬æ§åˆ¶ï¼ŒGitæ˜¯ä»£ç ç‰ˆæœ¬æ§åˆ¶
  - å­¦ä¹ DVCæ ¸å¿ƒæ¦‚å¿µï¼šæ•°æ®ç®¡é“ã€æ•°æ®æ³¨å†Œã€è¿œç¨‹å­˜å‚¨
  - è§‚çœ‹DVCå…¥é—¨è§†é¢‘ï¼šhttps://dvc.org/doc/start/data-versioning

**9:30-10:30 ç¯å¢ƒå‡†å¤‡**
- [ ] **æ£€æŸ¥Pythonç¯å¢ƒ**ï¼ˆ15åˆ†é’Ÿï¼‰
  ```bash
  # ç¡®è®¤Pythonç‰ˆæœ¬ï¼ˆéœ€è¦3.7+ï¼‰
  python --version
  
  # ç¡®è®¤pipç‰ˆæœ¬
  pip --version
  
  # å¦‚æœç‰ˆæœ¬è¿‡ä½ï¼Œå‡çº§pip
  python -m pip install --upgrade pip
  ```

- [ ] **å®‰è£…DVCåŠç›¸å…³ä¾èµ–**ï¼ˆ45åˆ†é’Ÿï¼‰
  ```bash
  # åŸºç¡€DVCå®‰è£…
  pip install dvc
  
  # å®‰è£…S3æ”¯æŒï¼ˆæ¨èç”¨äºäº‘å­˜å‚¨ï¼‰
  pip install dvc[s3]
  
  # å®‰è£…Google Driveæ”¯æŒï¼ˆå¯é€‰ï¼‰
  pip install dvc[gdrive]
  
  # å®‰è£…Azureæ”¯æŒï¼ˆå¯é€‰ï¼‰
  pip install dvc[azure]
  
  # éªŒè¯å®‰è£…
  dvc version
  ```

**10:30-11:00 ä¼‘æ¯**

**11:00-12:00 é¡¹ç›®åˆå§‹åŒ–**
- [ ] **åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„**ï¼ˆ20åˆ†é’Ÿï¼‰
  ```bash
  # åˆ›å»ºé¡¹ç›®æ ¹ç›®å½•
  mkdir drone_vision_project
  cd drone_vision_project
  
  # åˆ›å»ºåŸºç¡€ç›®å½•ç»“æ„
  mkdir -p {data,src,notebooks,docs,configs}
  mkdir -p data/{raw,processed,external}
  mkdir -p data/raw/{images,annotations,metadata}
  mkdir -p data/processed/{train,val,test}
  ```

- [ ] **åˆå§‹åŒ–Gitä»“åº“**ï¼ˆ20åˆ†é’Ÿï¼‰
  ```bash
  # åˆå§‹åŒ–Git
  git init
  
  # åˆ›å»º.gitignoreæ–‡ä»¶
  echo "*.pyc
  __pycache__/
  .DS_Store
  .env
  data/raw/*
  !data/raw/.gitkeep" > .gitignore
  
  # åˆ›å»ºREADME.md
  echo "# æ— äººæœºè§†è§‰ç®—æ³•å¹³å°" > README.md
  
  # é¦–æ¬¡æäº¤
  git add .
  git commit -m "Initial project setup"
  ```

- [ ] **åˆå§‹åŒ–DVCé¡¹ç›®**ï¼ˆ20åˆ†é’Ÿï¼‰
  ```bash
  # åˆå§‹åŒ–DVC
  dvc init
  
  # æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶
  ls -la .dvc/
  
  # æ·»åŠ DVCæ–‡ä»¶åˆ°Git
  git add .dvc
  git commit -m "Initialize DVC"
  
  # æŸ¥çœ‹DVCçŠ¶æ€
  dvc status
  ```

**ä¸‹åˆ (14:00-18:00) - å­˜å‚¨é…ç½®ä¸ç›®å½•ç»“æ„**

**14:00-15:00 è¿œç¨‹å­˜å‚¨é…ç½®**
- [ ] **å­¦ä¹ å­˜å‚¨é€‰é¡¹**ï¼ˆ20åˆ†é’Ÿï¼‰
  - äº†è§£DVCæ”¯æŒçš„å­˜å‚¨ç±»å‹ï¼šæœ¬åœ°ã€S3ã€GCSã€Azureã€SSHç­‰
  - é˜…è¯»å­˜å‚¨é…ç½®æ–‡æ¡£ï¼šhttps://dvc.org/doc/command-reference/remote
  - æ ¹æ®å…¬å¸ç¯å¢ƒé€‰æ‹©åˆé€‚çš„å­˜å‚¨æ–¹æ¡ˆ

- [ ] **é…ç½®æœ¬åœ°å­˜å‚¨**ï¼ˆ40åˆ†é’Ÿï¼‰
  ```bash
  # åˆ›å»ºæœ¬åœ°å­˜å‚¨ç›®å½•
  mkdir -p /path/to/dvc_storage
  
  # æ·»åŠ æœ¬åœ°è¿œç¨‹å­˜å‚¨
  dvc remote add -d storage /path/to/dvc_storage
  
  # æŸ¥çœ‹é…ç½®
  dvc remote list
  
  # æµ‹è¯•è¿æ¥
  dvc remote modify storage --local test true
  ```

**15:00-15:30 ä¼‘æ¯**

**15:30-16:30 äº‘å­˜å‚¨é…ç½®ï¼ˆå¯é€‰ï¼‰**
- [ ] **é…ç½®S3å­˜å‚¨**ï¼ˆ60åˆ†é’Ÿï¼‰
  ```bash
  # å®‰è£…AWS CLIï¼ˆå¦‚æœæœªå®‰è£…ï¼‰
  pip install awscli
  
  # é…ç½®AWSå‡­è¯
  aws configure
  # è¾“å…¥Access Key IDã€Secret Access Keyã€Region
  
  # åˆ›å»ºS3å­˜å‚¨æ¡¶ï¼ˆé€šè¿‡AWSæ§åˆ¶å°æˆ–CLIï¼‰
  aws s3 mb s3://your-drone-data-bucket
  
  # é…ç½®DVC S3è¿œç¨‹å­˜å‚¨
  dvc remote add -d storage s3://your-drone-data-bucket/drone-data
  
  # è®¾ç½®åŒºåŸŸ
  dvc remote modify storage region us-east-1
  
  # æµ‹è¯•è¿æ¥
  dvc remote modify storage --local test true
  ```

**16:30-17:30 ç›®å½•ç»“æ„å®Œå–„**
- [ ] **åˆ›å»ºè¯¦ç»†ç›®å½•ç»“æ„**ï¼ˆ30åˆ†é’Ÿï¼‰
  ```bash
  # åˆ›å»ºå®Œæ•´çš„æ•°æ®ç›®å½•ç»“æ„
  mkdir -p data/raw/{images/{train,val,test},annotations/{train,val,test},metadata}
  mkdir -p data/processed/{train,val,test,augmented}
  mkdir -p data/external/{visdrone,coco,imagenet}
  mkdir -p src/{data,models,utils,configs}
  mkdir -p notebooks/{exploration,training,evaluation}
  mkdir -p docs/{api,guides,reports}
  mkdir -p configs/{data,model,training}
  mkdir -p logs/{training,experiments,errors}
  mkdir -p checkpoints/{models,optimizers}
  mkdir -p outputs/{predictions,visualizations,reports}
  ```

- [ ] **åˆ›å»ºé…ç½®æ–‡ä»¶**ï¼ˆ30åˆ†é’Ÿï¼‰
  ```bash
  # åˆ›å»ºæ•°æ®é…ç½®æ–‡ä»¶
  cat > configs/data_config.yaml << EOF
  data:
    raw_path: "data/raw"
    processed_path: "data/processed"
    external_path: "data/external"
    
  datasets:
    visdrone:
      path: "data/external/visdrone"
      format: "coco"
    
  preprocessing:
    image_size: [640, 640]
    augmentation: true
    normalization: "imagenet"
  EOF
  
  # åˆ›å»ºæ¨¡å‹é…ç½®æ–‡ä»¶
  cat > configs/model_config.yaml << EOF
  model:
    architecture: "yolov5"
    num_classes: 10
    input_size: [640, 640]
    
  training:
    batch_size: 16
    learning_rate: 0.001
    epochs: 100
  EOF
  ```

**17:30-18:00 éªŒè¯ä¸æµ‹è¯•**
- [ ] **æµ‹è¯•DVCåŸºæœ¬åŠŸèƒ½**ï¼ˆ30åˆ†é’Ÿï¼‰
  ```bash
  # åˆ›å»ºæµ‹è¯•æ•°æ®
  echo "test data" > data/raw/test.txt
  
  # æ·»åŠ æ•°æ®åˆ°DVC
  dvc add data/raw/test.txt
  
  # æŸ¥çœ‹DVCçŠ¶æ€
  dvc status
  
  # æ¨é€åˆ°è¿œç¨‹å­˜å‚¨
  dvc push
  
  # åˆ é™¤æœ¬åœ°æ•°æ®
  rm data/raw/test.txt
  
  # ä»è¿œç¨‹æ‹‰å–æ•°æ®
  dvc pull
  
  # éªŒè¯æ•°æ®æ¢å¤
  cat data/raw/test.txt
  ```

#### å‘¨äºŒï¼šæ•°æ®å­˜å‚¨æ¶æ„è®¾è®¡

**ä¸Šåˆ (9:00-12:00) - å­˜å‚¨æ¶æ„ç†è®ºå­¦ä¹ ä¸è®¾è®¡**

**9:00-9:30 ç†è®ºå­¦ä¹ **
- [ ] **å­¦ä¹ æ•°æ®å­˜å‚¨æ¶æ„æ¦‚å¿µ**ï¼ˆ30åˆ†é’Ÿï¼‰
  - é˜…è¯»åˆ†å±‚å­˜å‚¨æ¶æ„æ–‡æ¡£ï¼šhttps://en.wikipedia.org/wiki/Tiered_storage
  - ç†è§£çƒ­å­˜å‚¨ã€æ¸©å­˜å‚¨ã€å†·å­˜å‚¨çš„åŒºåˆ«å’Œç”¨é€”
  çƒ­å­˜å‚¨ï¼šä¸ºæ€§èƒ½è€Œä¼˜åŒ–ã€ç‰ºç‰²æˆæœ¬ä»¥æ¢å–æè‡´çš„é€Ÿåº¦
  ç”¨é€”ï¼šAPPåŠ¨æ€å†…å®¹ã€åœ¨çº¿äº¤æ˜“ç³»ç»Ÿã€æ•°æ®åº“ã€å¤§æ•°æ®åˆ†æ
  æŠ€æœ¯è¦æ±‚ï¼šä½¿ç”¨æœ€å¿«çš„å­˜å‚¨ä»‹è´¨ï¼ˆå¦‚å›ºæ€ç¡¬ç›˜SSDï¼‰ç”šè‡³å†…å­˜ï¼Œç¡®ä¿æ•°æ®å»¶è¿Ÿæœ€ä½
  æ¸©å­˜å‚¨ï¼šåœ¨æ€§èƒ½å’Œæˆæœ¬ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œé€šå¸¸ä½¿ç”¨æ ‡å‡†æœºæ¢°ç¡¬ç›˜HDDæˆ–æ€§èƒ½ä¸æˆæœ¬å‡è¡¡çš„äº‘å­˜å‚¨æœåŠ¡å™¨
  å†·å­˜å‚¨ï¼šä¸ºæˆæœ¬è€Œä¼˜åŒ–ï¼Œç‰ºç‰²é€Ÿåº¦ä»¥æ¢å–æä½çš„å­˜å‚¨å•ä»·ï¼Œä½¿ç”¨æœ€ç»æµçš„ä»‹è´¨ï¼Œå¦‚ç£å¸¦ï¼Œè“å…‰å…‰ç›˜ã€‚
  - å­¦ä¹ æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†æ¦‚å¿µ
  æ¦‚å¿µï¼šæ•°æ®ä»åˆ›å»ºåˆ°æ¶ˆäº¡çš„æ•´ä¸ªæ—…ç¨‹è¿›è¡Œç®¡ç†çš„ä¸€å¥—ç­–ç•¥å’Œæµç¨‹ã€‚æ ¸å¿ƒæ€æƒ³ï¼šæ•°æ®åœ¨ä¸åŒé˜¶æ®µå…·æœ‰ä¸åŒçš„ä»·å€¼ã€ç”¨é€”å’Œé£é™©ï¼Œå› æ­¤åº”è¯¥æœ‰ä¸åŒçš„æ–¹æ³•å»ç®¡ç†ã€å­˜å‚¨å’Œä¿æŠ¤å®ƒã€‚
  - äº†è§£ä¸åŒå­˜å‚¨ä»‹è´¨çš„æ€§èƒ½ç‰¹ç‚¹ï¼ˆSSDã€HDDã€äº‘å­˜å‚¨ï¼‰
SSDï¼šæ ¸å¿ƒä¼˜åŠ¿ï¼šé€Ÿåº¦ä¸å»¶è¿Ÿ
ç”±äºæ²¡æœ‰æœºæ¢°éƒ¨ä»¶ï¼Œæ•°æ®é€šè¿‡ç”µä¿¡å·è®¿é—®ï¼Œå› æ­¤å…·æœ‰æä½çš„è®¿é—®å»¶è¿Ÿå’Œæé«˜çš„è¯»å†™é€Ÿåº¦ï¼ŒHDDæ— æ³•ä¸ä¹‹æ¯”æ‹Ÿ
æ ¸å¿ƒåŠ£åŠ¿ï¼šç›¸æ¯”HDDï¼ŒSSDçš„å•ä½å®¹é‡ä»·æ ¼ä»ç„¶æ˜‚è´µ
å…¸å‹åœºæ™¯ï¼šæ ¸å¿ƒä¸šåŠ¡æ•°æ®åº“
HDDï¼šä¼˜åŠ¿ï¼šå¯ä»¥ç”¨å¾ˆä½çš„æˆæœ¬è·å¾—å·¨å¤§çš„å­˜å‚¨ç©ºé—´ï¼ŒæŠ€æœ¯æˆç†Ÿç¨³å®š
å…¸å‹åº”ç”¨åœºæ™¯ï¼šæµ·é‡æ•°æ®å½’æ¡£ï¼ˆè§†é¢‘ã€å›¾ç‰‡ã€å¤‡ä»½ï¼‰ã€æ•°æ®ä¸­å¿ƒå†·æ•°æ®å­˜å‚¨
äº‘å­˜å‚¨ï¼šæ ¸å¿ƒä¼˜åŠ¿ï¼šå¼¹æ€§ä¸æœåŠ¡æ— é™æ‰©å±•æ€§å’Œé«˜å¯ç”¨æ€§æŒä¹…æ€§ä»¥åŠä¸°å¯Œçš„æœåŠ¡å±‚çº§
æ ¸å¿ƒåŠ£åŠ¿ï¼šå»¶è¿Ÿä¸æŒç»­æˆæœ¬
å…¸å‹åº”ç”¨åœºæ™¯ï¼šæ‰€æœ‰éœ€è¦å¼¹çª—çš„ä¸šåŠ¡ã€ç½‘ç«™å’Œwebåº”ç”¨æ‰˜ç®¡

æ€»ç»“ï¼šï¼ˆ1ï¼‰è¿½æ±‚æè‡´æ€§èƒ½ï¼Œä¸è®¡æˆæœ¬ï¼šSSD
ï¼ˆ2ï¼‰ï¼šéœ€è¦æµ·é‡å­˜å‚¨ä½†æ˜¯é¢„ç®—æœ‰é™ï¼Œä¸”å¯¹é€Ÿåº¦ä¸æ•æ„Ÿï¼šHDD
ï¼ˆ3ï¼šéœ€è¦çµæ´»æ€§ã€é«˜å¯ç”¨ã€å¹¶å¸Œæœ›å°†èµ„æœ¬æŒ‡å‡ºè½¬ä¸ºè¿è¥æ”¯å‡ºï¼šäº‘å­˜å‚¨
**9:30-10:30 éœ€æ±‚åˆ†æ**
- [ ] **åˆ†ææ— äººæœºæ•°æ®ç‰¹ç‚¹**ï¼ˆ30åˆ†é’Ÿï¼‰
  - æ•°æ®é‡ï¼šå•æ¬¡é£è¡Œå¯èƒ½äº§ç”Ÿå‡ GBåˆ°å‡ TBæ•°æ®
  - è®¿é—®é¢‘ç‡ï¼šå½“å‰è®­ç»ƒæ•°æ®é¢‘ç¹è®¿é—®ï¼Œå†å²æ•°æ®å¶å°”è®¿é—®
  - æ•°æ®ä»·å€¼ï¼šæ–°æ•°æ®ä»·å€¼é«˜ï¼Œæ—§æ•°æ®ä¸»è¦ç”¨äºå¤‡ä»½å’Œåˆè§„
  - å­˜å‚¨æˆæœ¬ï¼šéœ€è¦è€ƒè™‘å­˜å‚¨æˆæœ¬ä¸è®¿é—®æ€§èƒ½çš„å¹³è¡¡

- [ ] **è°ƒç ”å­˜å‚¨æ–¹æ¡ˆ**ï¼ˆ30åˆ†é’Ÿï¼‰
  - äº†è§£å…¬å¸ç°æœ‰å­˜å‚¨åŸºç¡€è®¾æ–½
  - è°ƒç ”äº‘å­˜å‚¨æœåŠ¡ï¼šAWS S3ã€é˜¿é‡Œäº‘OSSã€è…¾è®¯äº‘COS
  - äº†è§£æœ¬åœ°å­˜å‚¨é€‰é¡¹ï¼šNASã€SANã€åˆ†å¸ƒå¼å­˜å‚¨
  - è¯„ä¼°ä¸åŒæ–¹æ¡ˆçš„ä¼˜ç¼ºç‚¹å’Œæˆæœ¬

**10:30-11:00 ä¼‘æ¯**

**11:00-12:00 æ¶æ„è®¾è®¡**
- [ ] **è®¾è®¡åˆ†å±‚å­˜å‚¨æ¶æ„**ï¼ˆ60åˆ†é’Ÿï¼‰
  ```yaml
  # storage_architecture.yaml
  storage_layers:
    hot_storage:      # çƒ­å­˜å‚¨ - å½“å‰è®­ç»ƒæ•°æ®
      type: "SSD"
      capacity: "1TB"
      access_time: "< 1ms"
      cost_per_gb: 0.1
      use_cases: ["å½“å‰è®­ç»ƒæ•°æ®", "é¢‘ç¹è®¿é—®çš„æ¨¡å‹"]
      
    warm_storage:     # æ¸©å­˜å‚¨ - å†å²æ•°æ®
      type: "HDD"
      capacity: "10TB"
      access_time: "< 10ms"
      cost_per_gb: 0.03
      use_cases: ["å†å²è®­ç»ƒæ•°æ®", "æ¨¡å‹æ£€æŸ¥ç‚¹", "å®éªŒæ•°æ®"]
      
    cold_storage:     # å†·å­˜å‚¨ - å½’æ¡£æ•°æ®
      type: "S3/OSS"
      capacity: "100TB"
      access_time: "< 1s"
      cost_per_gb: 0.01
      use_cases: ["é•¿æœŸå½’æ¡£", "åˆè§„å­˜å‚¨", "å¤‡ä»½æ•°æ®"]
      
    cache_storage:    # ç¼“å­˜å­˜å‚¨ - ä¸´æ—¶æ•°æ®
      type: "Memory/Redis"
      capacity: "100GB"
      access_time: "< 0.1ms"
      use_cases: ["æ•°æ®é¢„å¤„ç†ç¼“å­˜", "æ¨¡å‹æ¨ç†ç¼“å­˜"]
  ```

**ä¸‹åˆ (14:00-18:00) - æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†å®ç°**

**14:00-15:00 ç”Ÿå‘½å‘¨æœŸç­–ç•¥è®¾è®¡**
- [ ] **è®¾è®¡æ•°æ®åˆ†ç±»è§„åˆ™**ï¼ˆ30åˆ†é’Ÿï¼‰
  ```python
  # data_classification.py
  class DataClassifier:
      def __init__(self):
          self.classification_rules = {
              'hot': {
                  'age_days': 7,
                  'access_frequency': 'daily',
                  'data_types': ['current_training', 'active_models']
              },
              'warm': {
                  'age_days': 30,
                  'access_frequency': 'weekly',
                  'data_types': ['historical_training', 'model_checkpoints']
              },
              'cold': {
                  'age_days': 90,
                  'access_frequency': 'monthly',
                  'data_types': ['archived_data', 'backup_data']
              }
          }
      
      def classify_data(self, data_path, metadata):
          """æ ¹æ®å…ƒæ•°æ®å¯¹æ•°æ®è¿›è¡Œåˆ†ç±»"""
          age_days = self.calculate_age(metadata['created_time'])
          access_count = metadata.get('access_count', 0)
          
          if age_days <= 7 and access_count > 10:
              return 'hot'
          elif age_days <= 30 and access_count > 5:
              return 'warm'
          else:
              return 'cold'
  ```

- [ ] **è®¾è®¡è¿ç§»ç­–ç•¥**ï¼ˆ30åˆ†é’Ÿï¼‰
  ```python
  # migration_strategy.py
  class MigrationStrategy:
      def __init__(self):
          self.migration_rules = {
              'hot_to_warm': {
                  'trigger': 'age_days > 7',
                  'action': 'move_to_warm_storage',
                  'schedule': 'daily'
              },
              'warm_to_cold': {
                  'trigger': 'age_days > 30',
                  'action': 'move_to_cold_storage',
                  'schedule': 'weekly'
              }
          }
  ```

**15:00-15:30 ä¼‘æ¯**

**15:30-16:30 ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨å®ç°**
- [ ] **å®ç°æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨**ï¼ˆ60åˆ†é’Ÿï¼‰
  ```python
  # data_lifecycle_manager.py
  import os
  import shutil
  import time
  from datetime import datetime, timedelta
  from pathlib import Path
  import boto3
  import yaml
  
  class DataLifecycleManager:
      def __init__(self, config_path="configs/storage_config.yaml"):
          with open(config_path, 'r') as f:
              self.config = yaml.safe_load(f)
          
          self.hot_storage = self.config['storage_layers']['hot_storage']['path']
          self.warm_storage = self.config['storage_layers']['warm_storage']['path']
          self.cold_storage = self.config['storage_layers']['cold_storage']['path']
          
          # åˆå§‹åŒ–S3å®¢æˆ·ç«¯ï¼ˆå¦‚æœä½¿ç”¨äº‘å­˜å‚¨ï¼‰
          if 's3' in self.cold_storage:
              self.s3_client = boto3.client('s3')
      
      def move_to_warm(self, data_path, age_days=7):
          """å°†è¶…è¿‡æŒ‡å®šå¤©æ•°çš„æ•°æ®ç§»è‡³æ¸©å­˜å‚¨"""
          try:
              # æ£€æŸ¥æ•°æ®å¹´é¾„
              if not self.is_older_than(data_path, age_days):
                  return False
              
              # åˆ›å»ºç›®æ ‡è·¯å¾„
              relative_path = self.get_relative_path(data_path, self.hot_storage)
              target_path = os.path.join(self.warm_storage, relative_path)
              os.makedirs(os.path.dirname(target_path), exist_ok=True)
              
              # ç§»åŠ¨æ•°æ®
              shutil.move(data_path, target_path)
              
              # æ›´æ–°å…ƒæ•°æ®
              self.update_metadata(target_path, {'moved_to_warm': datetime.now()})
              
              print(f"æ•°æ®å·²ç§»è‡³æ¸©å­˜å‚¨: {data_path} -> {target_path}")
              return True
              
          except Exception as e:
              print(f"ç§»åŠ¨æ•°æ®åˆ°æ¸©å­˜å‚¨å¤±è´¥: {e}")
              return False
      
      def move_to_cold(self, data_path, age_days=30):
          """å°†è¶…è¿‡æŒ‡å®šå¤©æ•°çš„æ•°æ®ç§»è‡³å†·å­˜å‚¨"""
          try:
              # æ£€æŸ¥æ•°æ®å¹´é¾„
              if not self.is_older_than(data_path, age_days):
                  return False
              
              # å¦‚æœæ˜¯S3å­˜å‚¨
              if 's3://' in self.cold_storage:
                  bucket_name = self.cold_storage.split('//')[1].split('/')[0]
                  key = self.get_relative_path(data_path, self.warm_storage)
                  
                  # ä¸Šä¼ åˆ°S3
                  self.s3_client.upload_file(data_path, bucket_name, key)
                  
                  # åˆ é™¤æœ¬åœ°æ–‡ä»¶
                  os.remove(data_path)
                  
                  print(f"æ•°æ®å·²ç§»è‡³å†·å­˜å‚¨: {data_path} -> s3://{bucket_name}/{key}")
              else:
                  # æœ¬åœ°å†·å­˜å‚¨
                  relative_path = self.get_relative_path(data_path, self.warm_storage)
                  target_path = os.path.join(self.cold_storage, relative_path)
                  os.makedirs(os.path.dirname(target_path), exist_ok=True)
                  shutil.move(data_path, target_path)
                  
                  print(f"æ•°æ®å·²ç§»è‡³å†·å­˜å‚¨: {data_path} -> {target_path}")
              
              return True
              
          except Exception as e:
              print(f"ç§»åŠ¨æ•°æ®åˆ°å†·å­˜å‚¨å¤±è´¥: {e}")
              return False
      
      def is_older_than(self, file_path, days):
          """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¶…è¿‡æŒ‡å®šå¤©æ•°"""
          file_time = os.path.getmtime(file_path)
          cutoff_time = time.time() - (days * 24 * 60 * 60)
          return file_time < cutoff_time
      
      def get_relative_path(self, file_path, base_path):
          """è·å–ç›¸å¯¹äºåŸºç¡€è·¯å¾„çš„ç›¸å¯¹è·¯å¾„"""
          return os.path.relpath(file_path, base_path)
      
      def update_metadata(self, file_path, metadata):
          """æ›´æ–°æ–‡ä»¶å…ƒæ•°æ®"""
          metadata_file = file_path + '.meta'
          with open(metadata_file, 'w') as f:
              yaml.dump(metadata, f)
      
      def run_lifecycle_management(self):
          """è¿è¡Œæ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
          print("å¼€å§‹æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†...")
          
          # å¤„ç†çƒ­å­˜å‚¨æ•°æ®
          for root, dirs, files in os.walk(self.hot_storage):
              for file in files:
                  if file.endswith('.meta'):
                      continue
                  file_path = os.path.join(root, file)
                  self.move_to_warm(file_path, age_days=7)
          
          # å¤„ç†æ¸©å­˜å‚¨æ•°æ®
          for root, dirs, files in os.walk(self.warm_storage):
              for file in files:
                  if file.endswith('.meta'):
                      continue
                  file_path = os.path.join(root, file)
                  self.move_to_cold(file_path, age_days=30)
          
          print("æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†å®Œæˆ")
  ```

**16:30-17:30 è‡ªåŠ¨åŒ–è„šæœ¬å®ç°**
- [ ] **åˆ›å»ºè‡ªåŠ¨åŒ–ç®¡ç†è„šæœ¬**ï¼ˆ60åˆ†é’Ÿï¼‰
  ```python
  # automate_lifecycle.py
  import schedule
  import time
  from data_lifecycle_manager import DataLifecycleManager
  
  def run_daily_cleanup():
      """æ¯æ—¥æ•°æ®æ¸…ç†ä»»åŠ¡"""
      manager = DataLifecycleManager()
      manager.run_lifecycle_management()
  
  def run_weekly_archive():
      """æ¯å‘¨æ•°æ®å½’æ¡£ä»»åŠ¡"""
      manager = DataLifecycleManager()
      # æ‰§è¡Œæ›´å½»åº•çš„å½’æ¡£ç­–ç•¥
      pass
  
  def main():
      # è®¾ç½®å®šæ—¶ä»»åŠ¡
      schedule.every().day.at("02:00").do(run_daily_cleanup)
      schedule.every().sunday.at("03:00").do(run_weekly_archive)
      
      print("æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†æœåŠ¡å·²å¯åŠ¨...")
      print("æ¯æ—¥2:00æ‰§è¡Œæ•°æ®æ¸…ç†")
      print("æ¯å‘¨æ—¥3:00æ‰§è¡Œæ•°æ®å½’æ¡£")
      
      while True:
          schedule.run_pending()
          time.sleep(60)
  
  if __name__ == "__main__":
      main()
  ```

**17:30-18:00 æµ‹è¯•ä¸éªŒè¯**
- [ ] **æµ‹è¯•ç”Ÿå‘½å‘¨æœŸç®¡ç†åŠŸèƒ½**ï¼ˆ30åˆ†é’Ÿï¼‰
  ```python
  # test_lifecycle.py
  import tempfile
  import os
  from data_lifecycle_manager import DataLifecycleManager
  
  def test_lifecycle_management():
      """æµ‹è¯•æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†åŠŸèƒ½"""
      # åˆ›å»ºä¸´æ—¶ç›®å½•
      with tempfile.TemporaryDirectory() as temp_dir:
          # åˆ›å»ºæµ‹è¯•æ•°æ®
          test_file = os.path.join(temp_dir, "test_data.txt")
          with open(test_file, 'w') as f:
              f.write("test data")
          
          # ä¿®æ”¹æ–‡ä»¶æ—¶é—´ï¼ˆæ¨¡æ‹Ÿæ—§æ–‡ä»¶ï¼‰
          old_time = time.time() - (8 * 24 * 60 * 60)  # 8å¤©å‰
          os.utime(test_file, (old_time, old_time))
          
          # æµ‹è¯•ç”Ÿå‘½å‘¨æœŸç®¡ç†
          manager = DataLifecycleManager()
          result = manager.move_to_warm(test_file, age_days=7)
          
          assert result == True, "æ•°æ®åº”è¯¥è¢«ç§»åŠ¨åˆ°æ¸©å­˜å‚¨"
          print("ç”Ÿå‘½å‘¨æœŸç®¡ç†æµ‹è¯•é€šè¿‡")
  
  if __name__ == "__main__":
      test_lifecycle_management()
  ```

#### å‘¨ä¸‰ï¼šæ•°æ®æ ‡æ³¨æµç¨‹è®¾è®¡

**ä¸Šåˆ (9:00-12:00) - æ ‡æ³¨æµç¨‹ç†è®ºå­¦ä¹ ä¸è®¾è®¡**

**9:00-9:30 ç†è®ºå­¦ä¹ **
- [ ] **å­¦ä¹ æ•°æ®æ ‡æ³¨åŸºç¡€çŸ¥è¯†**ï¼ˆ30åˆ†é’Ÿï¼‰
  - é˜…è¯»è®¡ç®—æœºè§†è§‰æ ‡æ³¨æŒ‡å—ï¼šhttps://www.robots.ox.ac.uk/~vgg/publications/2014/everingham14/everingham14.pdf
  - äº†è§£å¸¸è§æ ‡æ³¨æ ¼å¼ï¼šCOCOã€YOLOã€Pascal VOCã€LabelMe
  COCOï¼šç”±å¾®è½¯å‘å¸ƒï¼Œå·²ç»æˆä¸ºç›®æ ‡æ£€æµ‹ã€å®ä¾‹åˆ†å‰²å’Œå…³é”®ç‚¹æ£€æµ‹é¢†åŸŸçš„ä¸»æµæ ¼å¼ï¼Œæ‰€æœ‰çš„æ ‡æ³¨ä¿¡æ¯éƒ½å­˜å‚¨åœ¨ä¸€ä¸ªå·¨å¤§çš„JSONæ–‡ä»¶ä¸­
  ä¼˜ç‚¹ï¼šé«˜åº¦é›†æˆåŠŸèƒ½å¼ºå¤§é«˜æ•ˆ  ç¼ºç‚¹ï¼šJSONæ–‡ä»¶å¾ˆå¤§ï¼Œäººå·¥é˜…è¯»å’Œç¼–è¾‘å›°éš¾

  YOLOï¼šä¸€ç§æç®€æ–‡æœ¬æ ¼å¼ï¼Œæ€æƒ³æ˜¯ä½¿ç”¨å½’ä¸€åŒ–çš„åæ ‡ï¼Œç”¨txtæ ‡æ³¨æ–‡ä»¶
ä¼˜ç‚¹ï¼šæœºå™¨ä»…ä»æ™®ï¼Œå­˜å‚¨ç©ºé—´å°  ç¼ºç‚¹ï¼šä¿¡æ¯é‡å°‘ï¼Œè¯¡å¼‚åŒ–åæ ‡ä¸å¤Ÿç›´è§‚  ä¸»è¦ç”¨é€”ï¼šç›®æ ‡æ£€æµ‹
Labelme:äº¤äº’å¼å›¾åƒæ ‡æ³¨å·¥å…·ï¼Œæ¯å¼ å›¾ç‰‡å¯¹åº”ä¸€ä¸ªjsonæ³¨è§£æ–‡ä»¶
ä¼˜ç‚¹ï¼šçµæ´»ã€è‡ªåŒ…å«ã€å·¥å…·å‹å¥½ ç¼ºç‚¹ï¼šæ¯ä¸€å¼ å›¾ç‰‡ä¸€ä¸ªJSOPæ–‡ä»¶ï¼Œæ–‡ä»¶æ•°é‡å¤š ï¼Œæ ¼å¼ç›¸å¯¹éšæ„ï¼Œä¸å¦‚COCOæ ‡æ³¨åŒ–ï¼Œä¸é€‚åˆå¤§è§„æ¨¡å·¥ä¸šé¡¹ç›®
ä¸»è¦ç”¨é€”ï¼šè¯­ä¹‰åˆ†å‰²ï¼Œå®ä¾‹åˆ†å‰²ï¼Œå„ç§å½¢çŠ¶çš„æ ‡æ³¨


  - å­¦ä¹ æ ‡æ³¨è´¨é‡æ§åˆ¶æ–¹æ³•ï¼šå¤šäººæ ‡æ³¨ã€ä¸€è‡´æ€§æ£€æŸ¥ã€ä¸“å®¶å®¡æ ¸
  - è§‚çœ‹æ ‡æ³¨å·¥å…·æ¼”ç¤ºè§†é¢‘ï¼šLabelMeã€CVATã€Labelbox

**9:30-10:30 æ ‡æ³¨å·¥å…·è°ƒç ”**
- [ ] **è°ƒç ”å¼€æºæ ‡æ³¨å·¥å…·**ï¼ˆ30åˆ†é’Ÿï¼‰
  - **LabelMe**: ç®€å•æ˜“ç”¨ï¼Œé€‚åˆå°è§„æ¨¡é¡¹ç›®
    - å®‰è£…ï¼š`pip install labelme`
    - ä¼˜ç‚¹ï¼šè½»é‡çº§ï¼Œæ”¯æŒå¤šç§æ ¼å¼
    - ç¼ºç‚¹ï¼šåŠŸèƒ½ç›¸å¯¹ç®€å•
  - **CVAT**: åŠŸèƒ½å¼ºå¤§ï¼Œé€‚åˆå›¢é˜Ÿåä½œ
    - å®‰è£…ï¼šDockeréƒ¨ç½²
    - ä¼˜ç‚¹ï¼šæ”¯æŒå¤šäººåä½œï¼Œä»»åŠ¡ç®¡ç†
    - ç¼ºç‚¹ï¼šéƒ¨ç½²å¤æ‚ï¼Œèµ„æºæ¶ˆè€—å¤§
  - **Labelbox**: å•†ä¸šè§£å†³æ–¹æ¡ˆ
    - ä¼˜ç‚¹ï¼šåŠŸèƒ½å®Œæ•´ï¼Œäº‘ç«¯éƒ¨ç½²
    - ç¼ºç‚¹ï¼šéœ€è¦ä»˜è´¹

- [ ] **é€‰æ‹©æ ‡æ³¨å·¥å…·**ï¼ˆ30åˆ†é’Ÿï¼‰
  - æ ¹æ®é¡¹ç›®éœ€æ±‚é€‰æ‹©åˆé€‚çš„å·¥å…·
  - è€ƒè™‘å› ç´ ï¼šå›¢é˜Ÿè§„æ¨¡ã€æ•°æ®é‡ã€é¢„ç®—ã€æŠ€æœ¯è¦æ±‚
  - æ¨èï¼šå°é¡¹ç›®ç”¨LabelMeï¼Œå¤§é¡¹ç›®ç”¨CVAT

**10:30-11:00 ä¼‘æ¯**

**11:00-12:00 æ ‡æ³¨æµç¨‹è®¾è®¡**
- [ ] **è®¾è®¡æ ‡æ³¨å·¥ä½œæµ**ï¼ˆ60åˆ†é’Ÿï¼‰
  ```mermaid
  graph TD
      A[åŸå§‹å›¾åƒ] --> B[æ•°æ®é¢„å¤„ç†]
      B --> C[å›¾åƒè´¨é‡æ£€æŸ¥]
      C --> D{è´¨é‡åˆæ ¼?}
      D -->|å¦| E[å›¾åƒå¢å¼º/ä¿®å¤]
      E --> C
      D -->|æ˜¯| F[æ ‡æ³¨ä»»åŠ¡åˆ›å»º]
      F --> G[ä»»åŠ¡åˆ†é…ç»™æ ‡æ³¨å‘˜]
      G --> H[æ ‡æ³¨å‘˜æ ‡æ³¨]
      H --> I[åˆæ­¥è´¨é‡æ£€æŸ¥]
      I --> J{è´¨é‡åˆæ ¼?}
      J -->|å¦| K[è¿”å›é‡æ–°æ ‡æ³¨]
      K --> H
      J -->|æ˜¯| L[ä¸“å®¶å®¡æ ¸]
      L --> M{å®¡æ ¸é€šè¿‡?}
      M -->|å¦| N[ä¿®æ”¹å»ºè®®]
      N --> K
      M -->|æ˜¯| O[æ ‡æ³¨å®Œæˆ]
      O --> P[æ•°æ®å…¥åº“]
      P --> Q[ç‰ˆæœ¬æ§åˆ¶]
  ```

**ä¸‹åˆ (14:00-18:00) - æ ‡æ³¨å·¥å…·é…ç½®ä¸å®ç°**

**14:00-15:00 æ ‡æ³¨å·¥å…·ç¯å¢ƒæ­å»º**
- [ ] **å®‰è£…LabelMe**ï¼ˆ30åˆ†é’Ÿï¼‰
  ```bash
  # å®‰è£…LabelMe
  pip install labelme
  
  # éªŒè¯å®‰è£…
  labelme --version
  
  # å¯åŠ¨LabelMe
  labelme
  ```

- [ ] **å®‰è£…CVAT**ï¼ˆ30åˆ†é’Ÿï¼‰
  ```bash
  # å…‹éš†CVATä»“åº“
  git clone https://github.com/openvinotoolkit/cvat.git
  cd cvat
  
  # ä½¿ç”¨Docker Composeå¯åŠ¨
  docker-compose up -d
  
  # åˆ›å»ºè¶…çº§ç”¨æˆ·
  docker-compose exec cvat bash -ic 'python3 ~/manage.py createsuperuser'
  ```

**15:00-15:30 ä¼‘æ¯**

**15:30-16:30 æ ‡æ³¨é…ç½®å®ç°**
- [ ] **åˆ›å»ºæ ‡æ³¨å·¥å…·é…ç½®**ï¼ˆ60åˆ†é’Ÿï¼‰
  ```python
  # annotation_config.py
  import os
  from pathlib import Path
  
  class AnnotationConfig:
      def __init__(self):
          self.config = {
              "tools": {
                  "labelme": {
                      "enabled": True,
                      "path": "labelme",
                      "output_format": "json",
                      "supported_formats": ["COCO", "YOLO", "Pascal VOC"]
                  },
                  "cvat": {
                      "enabled": True,
                      "url": "http://localhost:8080",
                      "username": "admin",
                      "password": "admin",
                      "output_format": "COCO"
                  }
              },
              "formats": {
                  "COCO": {
                      "description": "COCOæ ¼å¼ï¼Œé€‚åˆç›®æ ‡æ£€æµ‹å’Œåˆ†å‰²",
                      "file_extension": ".json",
                      "structure": {
                          "images": [],
                          "annotations": [],
                          "categories": []
                      }
                  },
                  "YOLO": {
                      "description": "YOLOæ ¼å¼ï¼Œé€‚åˆç›®æ ‡æ£€æµ‹",
                      "file_extension": ".txt", 
                      "structure": "class_id center_x center_y width height"
                  },
                  "Pascal_VOC": {
                      "description": "Pascal VOCæ ¼å¼ï¼Œé€‚åˆç›®æ ‡æ£€æµ‹",
                      "file_extension": ".xml",
                      "structure": "XMLæ ¼å¼çš„æ ‡æ³¨æ–‡ä»¶"
                  }
              },
              "quality_control": {
                  "double_annotation": True,
                  "inter_annotator_agreement": 0.85,
                  "review_threshold": 0.9,
                  "expert_review": True,
                  "consensus_threshold": 0.8
              },
              "workflow": {
                  "preprocessing": {
                      "image_resize": True,
                      "max_size": [1920, 1080],
                      "quality_check": True
                  },
                  "annotation": {
                      "task_assignment": "round_robin",
                      "deadline_days": 3,
                      "priority_levels": ["high", "medium", "low"]
                  },
                  "postprocessing": {
                      "format_conversion": True,
                      "validation": True,
                      "version_control": True
                  }
              }
          }
      
      def get_tool_config(self, tool_name):
          """è·å–æŒ‡å®šå·¥å…·çš„é…ç½®"""
          return self.config["tools"].get(tool_name, {})
      
      def get_format_config(self, format_name):
          """è·å–æŒ‡å®šæ ¼å¼çš„é…ç½®"""
          return self.config["formats"].get(format_name, {})
      
      def validate_annotation(self, annotation_data, format_type):
          """éªŒè¯æ ‡æ³¨æ•°æ®æ ¼å¼"""
          format_config = self.get_format_config(format_type)
          # å®ç°æ ¼å¼éªŒè¯é€»è¾‘
          return True
  ```

**16:30-17:30 æ ‡æ³¨æµç¨‹è‡ªåŠ¨åŒ–**
- [ ] **å®ç°æ ‡æ³¨æµç¨‹è‡ªåŠ¨åŒ–**ï¼ˆ60åˆ†é’Ÿï¼‰
  ```python
  # annotation_workflow.py
  import os
  import json
  import subprocess
  from pathlib import Path
  from datetime import datetime
  import shutil
  
  class AnnotationWorkflow:
      def __init__(self, config_path="configs/annotation_config.yaml"):
          self.config = AnnotationConfig()
          self.workflow_log = []
      
      def preprocess_images(self, input_dir, output_dir):
          """å›¾åƒé¢„å¤„ç†"""
          print("å¼€å§‹å›¾åƒé¢„å¤„ç†...")
          
          # åˆ›å»ºè¾“å‡ºç›®å½•
          os.makedirs(output_dir, exist_ok=True)
          
          # å¤„ç†æ¯å¼ å›¾åƒ
          for img_file in Path(input_dir).glob("*.jpg"):
              # å›¾åƒè´¨é‡æ£€æŸ¥
              if self.check_image_quality(img_file):
                  # è°ƒæ•´å›¾åƒå¤§å°
                  processed_img = self.resize_image(img_file, max_size=(1920, 1080))
                  # ä¿å­˜å¤„ç†åçš„å›¾åƒ
                  shutil.copy2(processed_img, output_dir)
                  self.log_workflow(f"å¤„ç†å›¾åƒ: {img_file.name}")
              else:
                  self.log_workflow(f"è·³è¿‡ä½è´¨é‡å›¾åƒ: {img_file.name}")
          
          print(f"å›¾åƒé¢„å¤„ç†å®Œæˆï¼Œè¾“å‡ºåˆ°: {output_dir}")
      
      def create_annotation_tasks(self, image_dir, task_config):
          """åˆ›å»ºæ ‡æ³¨ä»»åŠ¡"""
          print("åˆ›å»ºæ ‡æ³¨ä»»åŠ¡...")
          
          # è·å–å›¾åƒåˆ—è¡¨
          image_files = list(Path(image_dir).glob("*.jpg"))
          
          # æŒ‰æ‰¹æ¬¡åˆ›å»ºä»»åŠ¡
          batch_size = task_config.get("batch_size", 10)
          for i in range(0, len(image_files), batch_size):
              batch_images = image_files[i:i+batch_size]
              task_id = f"task_{i//batch_size + 1}"
              
              # åˆ›å»ºä»»åŠ¡ç›®å½•
              task_dir = Path("annotation_tasks") / task_id
              task_dir.mkdir(parents=True, exist_ok=True)
              
              # å¤åˆ¶å›¾åƒåˆ°ä»»åŠ¡ç›®å½•
              for img_file in batch_images:
                  shutil.copy2(img_file, task_dir)
              
              # åˆ›å»ºä»»åŠ¡é…ç½®æ–‡ä»¶
              task_info = {
                  "task_id": task_id,
                  "created_time": datetime.now().isoformat(),
                  "image_count": len(batch_images),
                  "assigned_annotator": None,
                  "status": "pending",
                  "deadline": task_config.get("deadline_days", 3)
              }
              
              with open(task_dir / "task_info.json", "w") as f:
                  json.dump(task_info, f, indent=2)
              
              self.log_workflow(f"åˆ›å»ºä»»åŠ¡: {task_id}")
      
      def assign_tasks(self, annotators):
          """åˆ†é…æ ‡æ³¨ä»»åŠ¡"""
          print("åˆ†é…æ ‡æ³¨ä»»åŠ¡...")
          
          # è·å–æ‰€æœ‰å¾…åˆ†é…çš„ä»»åŠ¡
          task_dirs = list(Path("annotation_tasks").glob("task_*"))
          pending_tasks = []
          
          for task_dir in task_dirs:
              task_info_file = task_dir / "task_info.json"
              if task_info_file.exists():
                  with open(task_info_file, "r") as f:
                      task_info = json.load(f)
                      if task_info["status"] == "pending":
                          pending_tasks.append(task_info)
          
          # è½®è¯¢åˆ†é…ä»»åŠ¡
          for i, task in enumerate(pending_tasks):
              annotator = annotators[i % len(annotators)]
              task["assigned_annotator"] = annotator
              task["status"] = "assigned"
              
              # æ›´æ–°ä»»åŠ¡ä¿¡æ¯
              task_dir = Path("annotation_tasks") / task["task_id"]
              with open(task_dir / "task_info.json", "w") as f:
                  json.dump(task, f, indent=2)
              
              self.log_workflow(f"åˆ†é…ä»»åŠ¡ {task['task_id']} ç»™ {annotator}")
      
      def check_image_quality(self, image_path):
          """æ£€æŸ¥å›¾åƒè´¨é‡"""
          try:
              from PIL import Image
              import numpy as np
              
              img = Image.open(image_path)
              
              # æ£€æŸ¥å›¾åƒå°ºå¯¸
              if img.size[0] < 100 or img.size[1] < 100:
                  return False
              
              # æ£€æŸ¥å›¾åƒæ˜¯å¦æŸå
              img_array = np.array(img)
              if img_array.size == 0:
                  return False
              
              # æ£€æŸ¥å›¾åƒäº®åº¦ï¼ˆé¿å…è¿‡æš—æˆ–è¿‡äº®ï¼‰
              gray = img.convert('L')
              brightness = np.mean(gray)
              if brightness < 30 or brightness > 220:
                  return False
              
              return True
          except Exception as e:
              print(f"å›¾åƒè´¨é‡æ£€æŸ¥å¤±è´¥: {e}")
              return False
      
      def resize_image(self, image_path, max_size=(1920, 1080)):
          """è°ƒæ•´å›¾åƒå¤§å°"""
          from PIL import Image
          
          img = Image.open(image_path)
          img.thumbnail(max_size, Image.Resampling.LANCZOS)
          return img
      
      def log_workflow(self, message):
          """è®°å½•å·¥ä½œæµæ—¥å¿—"""
          timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
          log_entry = f"[{timestamp}] {message}"
          self.workflow_log.append(log_entry)
          print(log_entry)
      
      def export_workflow_log(self, output_file="workflow_log.txt"):
          """å¯¼å‡ºå·¥ä½œæµæ—¥å¿—"""
          with open(output_file, "w") as f:
              for entry in self.workflow_log:
                  f.write(entry + "\n")
          print(f"å·¥ä½œæµæ—¥å¿—å·²å¯¼å‡ºåˆ°: {output_file}")
  ```

**17:30-18:00 æµ‹è¯•ä¸éªŒè¯**
- [ ] **æµ‹è¯•æ ‡æ³¨æµç¨‹**ï¼ˆ30åˆ†é’Ÿï¼‰
  ```python
  # test_annotation_workflow.py
  import tempfile
  from annotation_workflow import AnnotationWorkflow
  
  def test_annotation_workflow():
      """æµ‹è¯•æ ‡æ³¨å·¥ä½œæµ"""
      # åˆ›å»ºä¸´æ—¶ç›®å½•
      with tempfile.TemporaryDirectory() as temp_dir:
          # åˆ›å»ºæµ‹è¯•å›¾åƒ
          test_images_dir = Path(temp_dir) / "test_images"
          test_images_dir.mkdir()
          
          # åˆ›å»ºä¸€äº›æµ‹è¯•å›¾åƒæ–‡ä»¶ï¼ˆè¿™é‡Œç”¨ç©ºæ–‡ä»¶æ¨¡æ‹Ÿï¼‰
          for i in range(5):
              (test_images_dir / f"test_{i}.jpg").touch()
          
          # æµ‹è¯•å·¥ä½œæµ
          workflow = AnnotationWorkflow()
          
          # æµ‹è¯•å›¾åƒé¢„å¤„ç†
          processed_dir = Path(temp_dir) / "processed"
          workflow.preprocess_images(test_images_dir, processed_dir)
          
          # æµ‹è¯•ä»»åŠ¡åˆ›å»º
          task_config = {"batch_size": 2, "deadline_days": 3}
          workflow.create_annotation_tasks(processed_dir, task_config)
          
          print("æ ‡æ³¨å·¥ä½œæµæµ‹è¯•å®Œæˆ")
  
  if __name__ == "__main__":
      test_annotation_workflow()
  ```

#### å‘¨å››ï¼šæ•°æ®å®‰å…¨æœºåˆ¶å»ºç«‹

**ä¸Šåˆ (9:00-12:00) - æ•°æ®å®‰å…¨ç†è®ºå­¦ä¹ ä¸åŠ å¯†å®ç°**

**9:00-9:30 ç†è®ºå­¦ä¹ **
- [ ] **å­¦ä¹ æ•°æ®å®‰å…¨åŸºç¡€çŸ¥è¯†**ï¼ˆ30åˆ†é’Ÿï¼‰
  - é˜…è¯»æ•°æ®å®‰å…¨æœ€ä½³å®è·µï¼šhttps://owasp.org/www-project-data-security-top-10/
  - äº†è§£åŠ å¯†ç®—æ³•ï¼šå¯¹ç§°åŠ å¯†ã€éå¯¹ç§°åŠ å¯†ã€å“ˆå¸Œå‡½æ•°
  å¯¹ç§°åŠ å¯†ï¼šæ ¸å¿ƒåŸç†ï¼šåŠ å¯†å’Œè§£å¯†ä½¿ç”¨ç›¸åŒçš„å¯†é’¥ å…³é”®æŒ‘æˆ˜ï¼šå¯†é’¥åˆ†å‘
  éå¯¹ç§°åŠ å¯†ï¼šä½¿ç”¨ä¸€å¯¹å¯†é’¥ï¼šå…¬é’¥å’Œç§é’¥
  å“ˆå¸Œå‡½æ•°ï¼šå°†ä»»æ„é•¿åº¦çš„è¾“å…¥æ•°æ®ï¼ˆå¦‚ä¸€ä¸ªæ–‡ä»¶ã€ä¸€æ®µæ–‡å­—ï¼‰é€šè¿‡ä¸€ä¸ªæ•°å­¦å‡½æ•°ï¼Œè½¬æ¢æˆå›ºå®šé•¿åº¦çš„ã€çœ‹ä¼¼éšæœºçš„å­—ç¬¦ä¸²
  - å­¦ä¹ è®¿é—®æ§åˆ¶æ¨¡å‹ï¼šRBACã€ABACã€DAC
  RBACï¼š åŸºäºè§’è‰²çš„è®¿é—®æ§åˆ¶ï¼Œæ ¸å¿ƒæ€æƒ³ï¼šå°†æƒé™åˆ†é…ç»™è§’è‰²ï¼Œå†å°†è§’è‰²åˆ†é…ç»™ç”¨æˆ·
  ABAC:åŸºäºå±æ€§çš„è®¿é—®æ§åˆ¶ï¼šä¸€ç§åŠ¨æ€çš„ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è®¿é—®æ§åˆ¶æ¨¡å‹ï¼Œé€šè¿‡è¯„ä¼°å±æ€§ã€ç­–ç•¥å’Œç¯å¢ƒæ¡ä»¶æ¥åšå‡ºè®¿é—®å†³ç­–
  DAC:è‡ªä¸»è®¿é—®æ§åˆ¶ï¼šå…¶ä¸­èµ„æºæ‰€æœ‰è€…ï¼ˆæˆ–åˆ›å»ºè€…ï¼‰å¯ä»¥å®Œå…¨æ§åˆ¶è¯¥èµ„æºï¼ŒåŒ…æ‹¬å†³å®šè°å¯ä»¥è®¿é—®è¯¥èµ„æºä»¥åŠå…·æœ‰ä½•ç§è®¿é—®æƒé™ï¼Œé€šå¸¸é€šè¿‡è®¿é—®æ§åˆ¶åˆ—è¡¨æ¥å®ç°
  - äº†è§£æ•°æ®è„±æ•æŠ€æœ¯ï¼šé™æ€è„±æ•ã€åŠ¨æ€è„±æ•
    é™æ€è„±æ•ï¼šé¡¾åæ€ä¹‰ï¼Œæ˜¯å¯¹é™æ­¢çš„ã€å­˜å‚¨çš„æ•°æ®è¿›è¡Œè„±æ•
    ä¸€æ¬¡æ€§ã€æ°¸ä¹…æ€§çš„å°†ç”Ÿäº§ç¯å¢ƒä¸­çš„æ•æ„Ÿæ•°æ®å˜å½¢ã€æ›¿æ¢æˆ–æ¨¡ç³ŠåŒ–ï¼Œç„¶åå°†è¿™ä»½å¤„ç†åçš„å‰¯æœ¬ç”¨äºéç”Ÿäº§ç¯å¢ƒ


    åŠ¨æ€è„±æ•ï¼šåœ¨æ•°æ®è¢«è®¿é—®çš„ç¬é—´è¿›è¡Œå®æ—¶è„±æ•
    ç”Ÿäº§åº“ä¸­çš„åŸå§‹æ•°æ®å§‹ç»ˆä¿æŒä¸å˜
**9:30-10:30 å®‰å…¨éœ€æ±‚åˆ†æ**
- [ ] **åˆ†ææ— äººæœºæ•°æ®å®‰å…¨éœ€æ±‚**ï¼ˆ30åˆ†é’Ÿï¼‰
  - æ•°æ®æ•æ„Ÿæ€§ï¼šå›¾åƒå¯èƒ½åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼ˆäººè„¸ã€è½¦ç‰Œã€ä½ç½®ï¼‰
  - åˆè§„è¦æ±‚ï¼šGDPRã€æ•°æ®ä¿æŠ¤æ³•ã€è¡Œä¸šæ ‡å‡†
  - è®¿é—®æ§åˆ¶ï¼šä¸åŒè§’è‰²éœ€è¦ä¸åŒæƒé™
  - å®¡è®¡è¦æ±‚ï¼šéœ€è¦è®°å½•æ‰€æœ‰æ•°æ®è®¿é—®å’Œä¿®æ”¹

- [ ] **è®¾è®¡å®‰å…¨ç­–ç•¥**ï¼ˆ30åˆ†é’Ÿï¼‰
  - æ•°æ®åˆ†ç±»ï¼šå…¬å¼€ã€å†…éƒ¨ã€æœºå¯†ã€ç»å¯†
  - è®¿é—®çº§åˆ«ï¼šåªè¯»ã€è¯»å†™ã€ç®¡ç†
  - åŠ å¯†ç­–ç•¥ï¼šä¼ è¾“åŠ å¯†ã€å­˜å‚¨åŠ å¯†ã€å¯†é’¥ç®¡ç†
  - å®¡è®¡ç­–ç•¥ï¼šè®¿é—®æ—¥å¿—ã€æ“ä½œè®°å½•ã€å¼‚å¸¸ç›‘æ§

**10:30-11:00 ä¼‘æ¯**

**11:00-12:00 æ•°æ®åŠ å¯†å®ç°**
- [ ] **å®ç°æ•°æ®åŠ å¯†ç³»ç»Ÿ**ï¼ˆ60åˆ†é’Ÿï¼‰
  ```python
  # data_encryption.py
  import os
  import base64
  from cryptography.fernet import Fernet
  from cryptography.hazmat.primitives import hashes
  from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
  import json
  from datetime import datetime
  
  class DataEncryption:
      def __init__(self, password=None):
          if password:
              self.key = self._derive_key(password)
          else:
              self.key = os.environ.get('ENCRYPTION_KEY')
              if not self.key:
                  self.key = Fernet.generate_key()
                  print(f"ç”Ÿæˆæ–°çš„åŠ å¯†å¯†é’¥ï¼Œè¯·ä¿å­˜: {self.key.decode()}")
          
          self.cipher = Fernet(self.key)
          self.encryption_log = []
      
      def _derive_key(self, password):
          """ä»å¯†ç æ´¾ç”ŸåŠ å¯†å¯†é’¥"""
          password_bytes = password.encode()
          salt = b'drone_vision_salt'  # åœ¨å®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨éšæœºç›
          kdf = PBKDF2HMAC(
              algorithm=hashes.SHA256(),
              length=32,
              salt=salt,
              iterations=100000,
          )
          key = base64.urlsafe_b64encode(kdf.derive(password_bytes))
          return key
      
      def encrypt_file(self, file_path, output_path=None):
          """åŠ å¯†æ–‡ä»¶"""
          try:
              # è¯»å–æ–‡ä»¶
              with open(file_path, 'rb') as f:
                  data = f.read()
              
              # åŠ å¯†æ•°æ®
              encrypted_data = self.cipher.encrypt(data)
              
              # ç¡®å®šè¾“å‡ºè·¯å¾„
              if output_path is None:
                  output_path = file_path + '.enc'
              
              # å†™å…¥åŠ å¯†æ–‡ä»¶
              with open(output_path, 'wb') as f:
                  f.write(encrypted_data)
              
              # è®°å½•åŠ å¯†æ—¥å¿—
              self.log_encryption('encrypt', file_path, output_path)
              
              print(f"æ–‡ä»¶å·²åŠ å¯†: {file_path} -> {output_path}")
              return output_path
              
          except Exception as e:
              print(f"æ–‡ä»¶åŠ å¯†å¤±è´¥: {e}")
              return None
      
      def decrypt_file(self, encrypted_path, output_path=None):
          """è§£å¯†æ–‡ä»¶"""
          try:
              # è¯»å–åŠ å¯†æ–‡ä»¶
              with open(encrypted_path, 'rb') as f:
                  encrypted_data = f.read()
              
              # è§£å¯†æ•°æ®
              decrypted_data = self.cipher.decrypt(encrypted_data)
              
              # ç¡®å®šè¾“å‡ºè·¯å¾„
              if output_path is None:
                  output_path = encrypted_path.replace('.enc', '')
              
              # å†™å…¥è§£å¯†æ–‡ä»¶
              with open(output_path, 'wb') as f:
                  f.write(decrypted_data)
              
              # è®°å½•è§£å¯†æ—¥å¿—
              self.log_encryption('decrypt', encrypted_path, output_path)
              
              print(f"æ–‡ä»¶å·²è§£å¯†: {encrypted_path} -> {output_path}")
              return output_path
              
          except Exception as e:
              print(f"æ–‡ä»¶è§£å¯†å¤±è´¥: {e}")
              return None
      
      def encrypt_directory(self, dir_path, output_dir=None):
          """åŠ å¯†æ•´ä¸ªç›®å½•"""
          if output_dir is None:
              output_dir = dir_path + '_encrypted'
          
          os.makedirs(output_dir, exist_ok=True)
          
          for root, dirs, files in os.walk(dir_path):
              for file in files:
                  file_path = os.path.join(root, file)
                  relative_path = os.path.relpath(file_path, dir_path)
                  output_path = os.path.join(output_dir, relative_path + '.enc')
                  
                  # åˆ›å»ºè¾“å‡ºç›®å½•
                  os.makedirs(os.path.dirname(output_path), exist_ok=True)
                  
                  # åŠ å¯†æ–‡ä»¶
                  self.encrypt_file(file_path, output_path)
      
      def decrypt_directory(self, encrypted_dir, output_dir=None):
          """è§£å¯†æ•´ä¸ªç›®å½•"""
          if output_dir is None:
              output_dir = encrypted_dir.replace('_encrypted', '_decrypted')
          
          os.makedirs(output_dir, exist_ok=True)
          
          for root, dirs, files in os.walk(encrypted_dir):
              for file in files:
                  if file.endswith('.enc'):
                      file_path = os.path.join(root, file)
                      relative_path = os.path.relpath(file_path, encrypted_dir)
                      output_path = os.path.join(output_dir, relative_path.replace('.enc', ''))
                      
                      # åˆ›å»ºè¾“å‡ºç›®å½•
                      os.makedirs(os.path.dirname(output_path), exist_ok=True)
                      
                      # è§£å¯†æ–‡ä»¶
                      self.decrypt_file(file_path, output_path)
      
      def log_encryption(self, operation, input_path, output_path):
          """è®°å½•åŠ å¯†æ“ä½œæ—¥å¿—"""
          log_entry = {
              'timestamp': datetime.now().isoformat(),
              'operation': operation,
              'input_path': input_path,
              'output_path': output_path,
              'user': os.getenv('USER', 'unknown')
          }
          self.encryption_log.append(log_entry)
      
      def export_encryption_log(self, output_file="encryption_log.json"):
          """å¯¼å‡ºåŠ å¯†æ—¥å¿—"""
          with open(output_file, 'w') as f:
              json.dump(self.encryption_log, f, indent=2)
          print(f"åŠ å¯†æ—¥å¿—å·²å¯¼å‡ºåˆ°: {output_file}")
  ```

**ä¸‹åˆ (14:00-18:00) - è®¿é—®æ§åˆ¶ä¸å®¡è®¡ç³»ç»Ÿ**

**14:00-15:00 è®¿é—®æ§åˆ¶ç³»ç»Ÿè®¾è®¡**
- [ ] **è®¾è®¡è®¿é—®æ§åˆ¶æ¨¡å‹**ï¼ˆ60åˆ†é’Ÿï¼‰
  ```python
  # access_control.py
  import json
  import hashlib
  from datetime import datetime, timedelta
  from enum import Enum
  
  class Permission(Enum):
      READ = "read"
      WRITE = "write"
      DELETE = "delete"
      MANAGE = "manage"
      ANNOTATE = "annotate"
      REVIEW = "review"
  
  class Role(Enum):
      ADMIN = "admin"
      ANNOTATOR = "annotator"
      REVIEWER = "reviewer"
      VIEWER = "viewer"
      DATA_SCIENTIST = "data_scientist"
  
  class AccessControl:
      def __init__(self, config_file="configs/access_control.json"):
          self.config_file = config_file
          self.permissions = self._load_permissions()
          self.users = self._load_users()
          self.access_log = []
      
      def _load_permissions(self):
          """åŠ è½½æƒé™é…ç½®"""
          default_permissions = {
              Role.ADMIN.value: [
                  Permission.READ.value,
                  Permission.WRITE.value,
                  Permission.DELETE.value,
                  Permission.MANAGE.value
              ],
              Role.ANNOTATOR.value: [
                  Permission.READ.value,
                  Permission.WRITE.value,
                  Permission.ANNOTATE.value
              ],
              Role.REVIEWER.value: [
                  Permission.READ.value,
                  Permission.REVIEW.value
              ],
              Role.VIEWER.value: [
                  Permission.READ.value
              ],
              Role.DATA_SCIENTIST.value: [
                  Permission.READ.value,
                  Permission.WRITE.value
              ]
          }
          
          try:
              with open(self.config_file, 'r') as f:
                  return json.load(f)
          except FileNotFoundError:
              return default_permissions
      
      def _load_users(self):
          """åŠ è½½ç”¨æˆ·é…ç½®"""
          try:
              with open("configs/users.json", 'r') as f:
                  return json.load(f)
          except FileNotFoundError:
              return {}
      
      def add_user(self, username, password, role, email=None):
          """æ·»åŠ ç”¨æˆ·"""
          user_id = self._generate_user_id(username)
          password_hash = self._hash_password(password)
          
          user_data = {
              'user_id': user_id,
              'username': username,
              'password_hash': password_hash,
              'role': role,
              'email': email,
              'created_at': datetime.now().isoformat(),
              'last_login': None,
              'is_active': True
          }
          
          self.users[user_id] = user_data
          self._save_users()
          
          print(f"ç”¨æˆ· {username} å·²æ·»åŠ ï¼Œè§’è‰²: {role}")
          return user_id
      
      def authenticate_user(self, username, password):
          """ç”¨æˆ·è®¤è¯"""
          for user_id, user_data in self.users.items():
              if user_data['username'] == username:
                  if self._verify_password(password, user_data['password_hash']):
                      # æ›´æ–°æœ€åç™»å½•æ—¶é—´
                      user_data['last_login'] = datetime.now().isoformat()
                      self._save_users()
                      return user_id
          return None
      
      def check_permission(self, user_id, action, resource=None):
          """æ£€æŸ¥ç”¨æˆ·æƒé™"""
          if user_id not in self.users:
              return False
          
          user_role = self.users[user_id]['role']
          user_permissions = self.permissions.get(user_role, [])
          
          has_permission = action in user_permissions
          
          # è®°å½•è®¿é—®æ—¥å¿—
          self.log_access(user_id, action, resource, has_permission)
          
          return has_permission
      
      def get_user_permissions(self, user_id):
          """è·å–ç”¨æˆ·æƒé™åˆ—è¡¨"""
          if user_id not in self.users:
              return []
          
          user_role = self.users[user_id]['role']
          return self.permissions.get(user_role, [])
      
      def _generate_user_id(self, username):
          """ç”Ÿæˆç”¨æˆ·ID"""
          return hashlib.md5(username.encode()).hexdigest()[:16]
      
      def _hash_password(self, password):
          """å“ˆå¸Œå¯†ç """
          salt = os.urandom(32)
          key = hashlib.pbkdf2_hmac('sha256', password.encode(), salt, 100000)
          return salt + key
      
      def _verify_password(self, password, stored_hash):
          """éªŒè¯å¯†ç """
          salt = stored_hash[:32]
          key = stored_hash[32:]
          new_key = hashlib.pbkdf2_hmac('sha256', password.encode(), salt, 100000)
          return key == new_key
      
      def log_access(self, user_id, action, resource, granted):
          """è®°å½•è®¿é—®æ—¥å¿—"""
          log_entry = {
              'timestamp': datetime.now().isoformat(),
              'user_id': user_id,
              'action': action,
              'resource': resource,
              'granted': granted,
              'ip_address': self._get_client_ip()
          }
          self.access_log.append(log_entry)
      
      def _get_client_ip(self):
          """è·å–å®¢æˆ·ç«¯IPï¼ˆç®€åŒ–å®ç°ï¼‰"""
          import socket
          return socket.gethostbyname(socket.gethostname())
      
      def _save_users(self):
          """ä¿å­˜ç”¨æˆ·æ•°æ®"""
          with open("configs/users.json", 'w') as f:
              json.dump(self.users, f, indent=2)
      
      def export_access_log(self, output_file="access_log.json"):
          """å¯¼å‡ºè®¿é—®æ—¥å¿—"""
          with open(output_file, 'w') as f:
              json.dump(self.access_log, f, indent=2)
          print(f"è®¿é—®æ—¥å¿—å·²å¯¼å‡ºåˆ°: {output_file}")
  ```

**15:00-15:30 ä¼‘æ¯**

**15:30-16:30 æ•°æ®è„±æ•å®ç°**
- [ ] **å®ç°æ•°æ®è„±æ•ç³»ç»Ÿ**ï¼ˆ60åˆ†é’Ÿï¼‰
  ```python
  # data_anonymization.py
  import cv2
  import numpy as np
  from PIL import Image, ImageFilter
  import json
  import hashlib
  from datetime import datetime
  
  class DataAnonymizer:
      def __init__(self):
          self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
          self.anonymization_log = []
      
      def anonymize_faces(self, image_path, output_path=None, method='blur'):
          """äººè„¸è„±æ•"""
          try:
              # è¯»å–å›¾åƒ
              image = cv2.imread(image_path)
              if image is None:
                  print(f"æ— æ³•è¯»å–å›¾åƒ: {image_path}")
                  return None
              
              # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
              gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
              
              # æ£€æµ‹äººè„¸
              faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)
              
              # å¯¹æ¯ä¸ªæ£€æµ‹åˆ°çš„äººè„¸è¿›è¡Œè„±æ•
              for (x, y, w, h) in faces:
                  if method == 'blur':
                      # æ¨¡ç³Šå¤„ç†
                      face_region = image[y:y+h, x:x+w]
                      blurred_face = cv2.GaussianBlur(face_region, (99, 99), 0)
                      image[y:y+h, x:x+w] = blurred_face
                  elif method == 'black':
                      # é»‘è‰²é®æŒ¡
                      cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 0), -1)
                  elif method == 'pixelate':
                      # åƒç´ åŒ–å¤„ç†
                      face_region = image[y:y+h, x:x+w]
                      small = cv2.resize(face_region, (10, 10))
                      pixelated = cv2.resize(small, (w, h), interpolation=cv2.INTER_NEAREST)
                      image[y:y+h, x:x+w] = pixelated
              
              # ä¿å­˜è„±æ•åçš„å›¾åƒ
              if output_path is None:
                  output_path = image_path.replace('.jpg', '_anonymized.jpg')
              
              cv2.imwrite(output_path, image)
              
              # è®°å½•è„±æ•æ—¥å¿—
              self.log_anonymization('face', image_path, output_path, len(faces))
              
              print(f"äººè„¸è„±æ•å®Œæˆ: {image_path} -> {output_path}")
              return output_path
              
          except Exception as e:
              print(f"äººè„¸è„±æ•å¤±è´¥: {e}")
              return None
      
      def anonymize_metadata(self, metadata, sensitive_fields=None):
          """å…ƒæ•°æ®è„±æ•"""
          if sensitive_fields is None:
              sensitive_fields = [
                  'gps_coordinates', 'timestamp', 'device_id', 
                  'user_id', 'location', 'camera_serial'
              ]
          
          anonymized_metadata = metadata.copy()
          
          for field in sensitive_fields:
              if field in anonymized_metadata:
                  if field == 'gps_coordinates':
                      # GPSåæ ‡æ¨¡ç³ŠåŒ–ï¼ˆä¿ç•™å¤§æ¦‚ä½ç½®ï¼‰
                      coords = anonymized_metadata[field]
                      if isinstance(coords, list) and len(coords) >= 2:
                          # å°†åæ ‡ç²¾åº¦é™ä½åˆ°100ç±³çº§åˆ«
                          lat = round(coords[0], 2)
                          lon = round(coords[1], 2)
                          anonymized_metadata[field] = [lat, lon]
                  elif field == 'timestamp':
                      # æ—¶é—´æˆ³æ¨¡ç³ŠåŒ–ï¼ˆä¿ç•™æ—¥æœŸï¼Œéšè—å…·ä½“æ—¶é—´ï¼‰
                      timestamp = anonymized_metadata[field]
                      if isinstance(timestamp, str):
                          date_part = timestamp.split('T')[0]
                          anonymized_metadata[field] = f"{date_part}T00:00:00Z"
                  else:
                      # å…¶ä»–æ•æ„Ÿå­—æ®µç”¨å“ˆå¸Œå€¼æ›¿ä»£
                      original_value = str(anonymized_metadata[field])
                      hashed_value = hashlib.sha256(original_value.encode()).hexdigest()[:8]
                      anonymized_metadata[field] = f"***{hashed_value}***"
          
          # è®°å½•è„±æ•æ—¥å¿—
          self.log_anonymization('metadata', 'metadata', 'anonymized_metadata', len(sensitive_fields))
          
          return anonymized_metadata
      
      def anonymize_text(self, text, sensitive_patterns=None):
          """æ–‡æœ¬è„±æ•"""
          if sensitive_patterns is None:
              sensitive_patterns = [
                  r'\b\d{11}\b',  # æ‰‹æœºå·
                  r'\b\d{18}\b',  # èº«ä»½è¯å·
                  r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',  # é‚®ç®±
                  r'\b\d{4}-\d{2}-\d{2}\b'  # æ—¥æœŸ
              ]
          
          import re
          anonymized_text = text
          
          for pattern in sensitive_patterns:
              matches = re.findall(pattern, text)
              for match in matches:
                  # ç”¨æ˜Ÿå·æ›¿æ¢æ•æ„Ÿä¿¡æ¯
                  replacement = '*' * len(match)
                  anonymized_text = anonymized_text.replace(match, replacement)
          
          return anonymized_text
      
      def log_anonymization(self, data_type, input_path, output_path, count):
          """è®°å½•è„±æ•æ—¥å¿—"""
          log_entry = {
              'timestamp': datetime.now().isoformat(),
              'data_type': data_type,
              'input_path': input_path,
              'output_path': output_path,
              'anonymized_count': count
          }
          self.anonymization_log.append(log_entry)
      
      def export_anonymization_log(self, output_file="anonymization_log.json"):
          """å¯¼å‡ºè„±æ•æ—¥å¿—"""
          with open(output_file, 'w') as f:
              json.dump(self.anonymization_log, f, indent=2)
          print(f"è„±æ•æ—¥å¿—å·²å¯¼å‡ºåˆ°: {output_file}")
  ```

**16:30-17:30 å®¡è®¡ç³»ç»Ÿå®ç°**
- [ ] **å®ç°å®¡è®¡æ—¥å¿—ç³»ç»Ÿ**ï¼ˆ60åˆ†é’Ÿï¼‰
  ```python
  # audit_system.py
  import json
  import logging
  from datetime import datetime
  from pathlib import Path
  import hashlib
  
  class AuditSystem:
      def __init__(self, log_dir="logs/audit"):
          self.log_dir = Path(log_dir)
          self.log_dir.mkdir(parents=True, exist_ok=True)
          
          # é…ç½®æ—¥å¿—
          self.logger = logging.getLogger('audit_system')
          self.logger.setLevel(logging.INFO)
          
          # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
          log_file = self.log_dir / f"audit_{datetime.now().strftime('%Y%m%d')}.log"
          file_handler = logging.FileHandler(log_file)
          file_handler.setLevel(logging.INFO)
          
          # åˆ›å»ºæ ¼å¼å™¨
          formatter = logging.Formatter(
              '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
          )
          file_handler.setFormatter(formatter)
          
          self.logger.addHandler(file_handler)
      
      def log_data_access(self, user_id, data_path, action, result):
          """è®°å½•æ•°æ®è®¿é—®"""
          log_entry = {
              'event_type': 'data_access',
              'timestamp': datetime.now().isoformat(),
              'user_id': user_id,
              'data_path': data_path,
              'action': action,
              'result': result,
              'ip_address': self._get_client_ip()
          }
          
          self.logger.info(f"æ•°æ®è®¿é—®: {json.dumps(log_entry)}")
          return log_entry
      
      def log_data_modification(self, user_id, data_path, changes, before_hash, after_hash):
          """è®°å½•æ•°æ®ä¿®æ”¹"""
          log_entry = {
              'event_type': 'data_modification',
              'timestamp': datetime.now().isoformat(),
              'user_id': user_id,
              'data_path': data_path,
              'changes': changes,
              'before_hash': before_hash,
              'after_hash': after_hash,
              'ip_address': self._get_client_ip()
          }
          
          self.logger.info(f"æ•°æ®ä¿®æ”¹: {json.dumps(log_entry)}")
          return log_entry
      
      def log_security_event(self, event_type, user_id, description, severity='medium'):
          """è®°å½•å®‰å…¨äº‹ä»¶"""
          log_entry = {
              'event_type': 'security_event',
              'timestamp': datetime.now().isoformat(),
              'user_id': user_id,
              'description': description,
              'severity': severity,
              'ip_address': self._get_client_ip()
          }
          
          if severity == 'high':
              self.logger.error(f"å®‰å…¨äº‹ä»¶: {json.dumps(log_entry)}")
          else:
              self.logger.warning(f"å®‰å…¨äº‹ä»¶: {json.dumps(log_entry)}")
          
          return log_entry
      
      def log_system_event(self, event_type, description, details=None):
          """è®°å½•ç³»ç»Ÿäº‹ä»¶"""
          log_entry = {
              'event_type': 'system_event',
              'timestamp': datetime.now().isoformat(),
              'description': description,
              'details': details,
              'ip_address': self._get_client_ip()
          }
          
          self.logger.info(f"ç³»ç»Ÿäº‹ä»¶: {json.dumps(log_entry)}")
          return log_entry
      
      def _get_client_ip(self):
          """è·å–å®¢æˆ·ç«¯IP"""
          import socket
          return socket.gethostbyname(socket.gethostname())
      
      def generate_audit_report(self, start_date, end_date, output_file=None):
          """ç”Ÿæˆå®¡è®¡æŠ¥å‘Š"""
          if output_file is None:
              output_file = self.log_dir / f"audit_report_{start_date}_{end_date}.json"
          
          # è¿™é‡Œåº”è¯¥å®ç°ä»æ—¥å¿—æ–‡ä»¶ä¸­æå–å’Œæ±‡æ€»æ•°æ®
          # ç®€åŒ–å®ç°
          report = {
              'report_period': f"{start_date} to {end_date}",
              'generated_at': datetime.now().isoformat(),
              'summary': {
                  'total_events': 0,
                  'data_access_events': 0,
                  'modification_events': 0,
                  'security_events': 0
              }
          }
          
          with open(output_file, 'w') as f:
              json.dump(report, f, indent=2)
          
          print(f"å®¡è®¡æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
          return report
  ```

**17:30-18:00 å®‰å…¨ç³»ç»Ÿæµ‹è¯•**
- [ ] **æµ‹è¯•å®‰å…¨ç³»ç»ŸåŠŸèƒ½**ï¼ˆ30åˆ†é’Ÿï¼‰
  ```python
  # test_security_system.py
  import tempfile
  import os
  from data_encryption import DataEncryption
  from access_control import AccessControl
  from data_anonymization import DataAnonymizer
  from audit_system import AuditSystem
  
  def test_security_system():
      """æµ‹è¯•å®‰å…¨ç³»ç»ŸåŠŸèƒ½"""
      print("å¼€å§‹æµ‹è¯•å®‰å…¨ç³»ç»Ÿ...")
      
      # æµ‹è¯•æ•°æ®åŠ å¯†
      print("\n1. æµ‹è¯•æ•°æ®åŠ å¯†...")
      with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:
          f.write("æµ‹è¯•æ•°æ®")
          test_file = f.name
      
      encryption = DataEncryption()
      encrypted_file = encryption.encrypt_file(test_file)
      assert encrypted_file is not None, "åŠ å¯†å¤±è´¥"
      
      decrypted_file = encryption.decrypt_file(encrypted_file)
      assert decrypted_file is not None, "è§£å¯†å¤±è´¥"
      
      # æµ‹è¯•è®¿é—®æ§åˆ¶
      print("\n2. æµ‹è¯•è®¿é—®æ§åˆ¶...")
      access_control = AccessControl()
      user_id = access_control.add_user("test_user", "password123", "annotator")
      
      # æµ‹è¯•æƒé™æ£€æŸ¥
      can_read = access_control.check_permission(user_id, "read")
      can_delete = access_control.check_permission(user_id, "delete")
      
      assert can_read == True, "è¯»å–æƒé™æ£€æŸ¥å¤±è´¥"
      assert can_delete == False, "åˆ é™¤æƒé™æ£€æŸ¥å¤±è´¥"
      
      # æµ‹è¯•å®¡è®¡ç³»ç»Ÿ
      print("\n3. æµ‹è¯•å®¡è®¡ç³»ç»Ÿ...")
      audit = AuditSystem()
      audit.log_data_access(user_id, test_file, "read", "success")
      audit.log_security_event("unauthorized_access", user_id, "æµ‹è¯•å®‰å…¨äº‹ä»¶")
      
      print("\nå®‰å…¨ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")
      
      # æ¸…ç†æµ‹è¯•æ–‡ä»¶
      os.unlink(test_file)
      if os.path.exists(encrypted_file):
          os.unlink(encrypted_file)
      if os.path.exists(decrypted_file):
          os.unlink(decrypted_file)
  
  if __name__ == "__main__":
      test_security_system()
  ```

#### å‘¨äº”ï¼šéšç§ä¿æŠ¤æœºåˆ¶

**ä¸Šåˆ (9:00-12:00)**
- [ ] å®ç°æ•°æ®è„±æ•
  ```python
  # data_anonymization.py
  import cv2
  import numpy as np
  
  class DataAnonymizer:
      def __init__(self):
          self.face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
      
      def anonymize_faces(self, image):
          """äººè„¸è„±æ•"""
          gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
          faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)
          
          for (x, y, w, h) in faces:
              cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 0), -1)
          
          return image
      
      def anonymize_metadata(self, metadata):
          """å…ƒæ•°æ®è„±æ•"""
          sensitive_fields = ['gps_coordinates', 'timestamp', 'device_id']
          for field in sensitive_fields:
              if field in metadata:
                  metadata[field] = "***"
          return metadata
  ```

**ä¸‹åˆ (14:00-18:00) - ç³»ç»Ÿé›†æˆä¸æµ‹è¯•**

**14:00-15:00 ç³»ç»Ÿé›†æˆ**
- [ ] **é›†æˆæ‰€æœ‰æ•°æ®ç®¡ç†ç»„ä»¶**ï¼ˆ60åˆ†é’Ÿï¼‰
  ```python
  # data_management_system.py
  from data_encryption import DataEncryption
  from access_control import AccessControl
  from data_anonymization import DataAnonymizer
  from audit_system import AuditSystem
  from data_lifecycle_manager import DataLifecycleManager
  import dvc.api
  
  class DataManagementSystem:
      def __init__(self):
          self.encryption = DataEncryption()
          self.access_control = AccessControl()
          self.anonymizer = DataAnonymizer()
          self.audit = AuditSystem()
          self.lifecycle_manager = DataLifecycleManager()
      
      def process_new_data(self, data_path, user_id, data_category="internal"):
          """å¤„ç†æ–°æ•°æ®"""
          # 1. æ£€æŸ¥ç”¨æˆ·æƒé™
          if not self.access_control.check_permission(user_id, "write", data_path):
              return False
          
          # 2. æ•°æ®è„±æ•ï¼ˆå¦‚æœéœ€è¦ï¼‰
          if data_category in ["confidential", "top_secret"]:
              data_path = self.anonymizer.anonymize_faces(data_path)
          
          # 3. åŠ å¯†æ•°æ®
          encrypted_path = self.encryption.encrypt_file(data_path)
          
          # 4. æ·»åŠ åˆ°DVCç‰ˆæœ¬æ§åˆ¶
          dvc.api.add(encrypted_path)
          
          # 5. è®°å½•å®¡è®¡æ—¥å¿—
          self.audit.log_data_access(user_id, data_path, "write", "success")
          
          return True
  ```

**15:00-15:30 ä¼‘æ¯**

**15:30-16:30 ç³»ç»Ÿæµ‹è¯•**
- [ ] **å…¨é¢æµ‹è¯•æ•°æ®ç®¡ç†ç³»ç»Ÿ**ï¼ˆ60åˆ†é’Ÿï¼‰
  ```python
  # test_data_management_system.py
  import tempfile
  import os
  from data_management_system import DataManagementSystem
  
  def test_data_management_system():
      """æµ‹è¯•æ•°æ®ç®¡ç†ç³»ç»Ÿ"""
      print("å¼€å§‹æµ‹è¯•æ•°æ®ç®¡ç†ç³»ç»Ÿ...")
      
      with tempfile.TemporaryDirectory() as temp_dir:
          system = DataManagementSystem()
          
          # æ·»åŠ æµ‹è¯•ç”¨æˆ·
          user_id = system.access_control.add_user("test_user", "password123", "annotator")
          
          # åˆ›å»ºæµ‹è¯•æ•°æ®
          test_data_file = os.path.join(temp_dir, "test_data.json")
          with open(test_data_file, 'w') as f:
              f.write('{"test": "data"}')
          
          # æµ‹è¯•æ•°æ®å¤„ç†
          result = system.process_new_data(test_data_file, user_id, "internal")
          assert result == True, "æ•°æ®å¤„ç†å¤±è´¥"
          
          print("æ•°æ®ç®¡ç†ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")
  
  if __name__ == "__main__":
      test_data_management_system()
  ```

**16:30-17:30 æ–‡æ¡£ç¼–å†™**
- [ ] **ç¼–å†™ç³»ç»Ÿæ–‡æ¡£**ï¼ˆ60åˆ†é’Ÿï¼‰
  - åˆ›å»ºæ•°æ®ç®¡ç†ç³»ç»Ÿä½¿ç”¨æŒ‡å—
  - ç¼–å†™APIæ–‡æ¡£http://127.0.0.1:5000
  - è®°å½•é…ç½®è¯´æ˜
  - åˆ›å»ºæ•…éšœæ’é™¤æŒ‡å—

**17:30-18:00 éªŒæ”¶æµ‹è¯•**
- [ ] **æœ€ç»ˆéªŒæ”¶æµ‹è¯•**ï¼ˆ30åˆ†é’Ÿï¼‰
  - éªŒè¯æ‰€æœ‰åŠŸèƒ½æ¨¡å—æ­£å¸¸å·¥ä½œ
  - æ£€æŸ¥ç³»ç»Ÿé›†æˆæ˜¯å¦å®Œæ•´
  - ç¡®è®¤æ–‡æ¡£æ˜¯å¦é½å…¨
  - æµ‹è¯•å¼‚å¸¸æƒ…å†µå¤„ç†

---

## ç¬¬4å‘¨ï¼šå®Œæ•´è®­ç»ƒæµç¨‹æµ‹è¯•

### ç¬¬4å‘¨ï¼šå®Œæ•´è®­ç»ƒæµç¨‹æµ‹è¯•

#### å‘¨ä¸€ï¼šå°è§„æ¨¡æ•°æ®é›†å‡†å¤‡

**ä¸Šåˆ (9:00-12:00) - æ•°æ®é›†ä¸‹è½½ä¸é¢„å¤„ç†**

**9:00-9:30 ç†è®ºå­¦ä¹ **
- [ ] **å­¦ä¹ VisDroneæ•°æ®é›†**ï¼ˆ30åˆ†é’Ÿï¼‰
  - é˜…è¯»VisDroneæ•°æ®é›†è®ºæ–‡ï¼šhttps://arxiv.org/abs/1804.07441
  - äº†è§£æ•°æ®é›†ç»“æ„ï¼šè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
  - å­¦ä¹ æ ‡æ³¨æ ¼å¼ï¼šCOCOæ ¼å¼ã€VisDroneæ ¼å¼
  - äº†è§£æ•°æ®ç‰¹ç‚¹ï¼šæ— äººæœºè§†è§’ã€å¤šå°ºåº¦ç›®æ ‡ã€å¤æ‚åœºæ™¯

**9:30-10:30 æ•°æ®é›†ä¸‹è½½**
- [ ] **ä¸‹è½½VisDroneæ•°æ®é›†**ï¼ˆ60åˆ†é’Ÿï¼‰
  ```bash
  # åˆ›å»ºæ•°æ®é›†ç›®å½•
  mkdir -p data/external/visdrone
  cd data/external/visdrone
  
  # ä¸‹è½½æ•°æ®é›†ï¼ˆéœ€è¦æ³¨å†ŒVisDroneå®˜ç½‘ï¼‰
  git clone https://github.com/VisDrone/VisDrone-Dataset.git
  
  # è§£å‹æ•°æ®é›†æ–‡ä»¶
  unzip VisDrone2019-DET-train.zip
  unzip VisDrone2019-DET-val.zip
  unzip VisDrone2019-DET-test-dev.zip
  ```

**10:30-11:00 ä¼‘æ¯**

**11:00-12:00 æ•°æ®é¢„å¤„ç†**
- [ ] **å®ç°æ•°æ®é¢„å¤„ç†è„šæœ¬**ï¼ˆ60åˆ†é’Ÿï¼‰
  ```python
  # visdrone_processor.py
  import os
  import json
  from pathlib import Path
  import cv2
  from PIL import Image
  
  class VisDroneProcessor:
      def __init__(self, data_path="data/external/visdrone"):
          self.data_path = Path(data_path)
          self.output_path = Path("data/processed/visdrone")
          self.class_names = [
              'ignored regions', 'pedestrian', 'people', 'bicycle', 'car', 
              'van', 'truck', 'tricycle', 'awning-tricycle', 'bus', 'motor'
          ]
      
      def process_dataset(self):
          """å¤„ç†æ•´ä¸ªæ•°æ®é›†"""
          print("å¼€å§‹å¤„ç†VisDroneæ•°æ®é›†...")
          
          # åˆ›å»ºè¾“å‡ºç›®å½•
          self.output_path.mkdir(parents=True, exist_ok=True)
          
          # å¤„ç†è®­ç»ƒé›†
          self.process_split("train")
          
          # å¤„ç†éªŒè¯é›†
          self.process_split("val")
          
          print("æ•°æ®é›†å¤„ç†å®Œæˆ")
      
      def process_split(self, split_name):
          """å¤„ç†ç‰¹å®šåˆ†å‰²çš„æ•°æ®"""
          print(f"å¤„ç†{split_name}é›†...")
          
          # åˆ›å»ºåˆ†å‰²ç›®å½•
          split_dir = self.output_path / split_name
          split_dir.mkdir(exist_ok=True)
          
          # å¤„ç†å›¾åƒ
          images_dir = split_dir / "images"
          images_dir.mkdir(exist_ok=True)
          
          # å¤„ç†æ ‡æ³¨
          annotations_dir = split_dir / "annotations"
          annotations_dir.mkdir(exist_ok=True)
          
          # å¤åˆ¶å’Œè½¬æ¢å›¾åƒ
          source_images_dir = self.data_path / f"VisDrone2019-DET-{split_name}" / "images"
          if source_images_dir.exists():
              for img_file in source_images_dir.glob("*.jpg"):
                  # è°ƒæ•´å›¾åƒå¤§å°
                  processed_img = self.resize_image(img_file, max_size=(640, 640))
                  processed_img.save(images_dir / img_file.name)
          
          # è½¬æ¢æ ‡æ³¨æ ¼å¼
          source_annotations_dir = self.data_path / f"VisDrone2019-DET-{split_name}" / "annotations"
          if source_annotations_dir.exists():
              coco_annotations = self.convert_to_coco_format(
                  source_annotations_dir, images_dir, split_name
              )
              
              # ä¿å­˜COCOæ ¼å¼æ ‡æ³¨
              with open(annotations_dir / "annotations.json", 'w') as f:
                  json.dump(coco_annotations, f, indent=2)
      
      def resize_image(self, image_path, max_size=(640, 640)):
          """è°ƒæ•´å›¾åƒå¤§å°"""
          img = Image.open(image_path)
          img.thumbnail(max_size, Image.Resampling.LANCZOS)
          return img
      
      def convert_to_coco_format(self, annotations_dir, images_dir, split_name):
          """è½¬æ¢ä¸ºCOCOæ ¼å¼"""
          coco_data = {
              "images": [],
              "annotations": [],
              "categories": []
          }
          
          # æ·»åŠ ç±»åˆ«ä¿¡æ¯
          for i, class_name in enumerate(self.class_names):
              coco_data["categories"].append({
                  "id": i,
                  "name": class_name,
                  "supercategory": "object"
              })
          
          image_id = 0
          annotation_id = 0
          
          # å¤„ç†æ¯ä¸ªå›¾åƒ
          for img_file in images_dir.glob("*.jpg"):
              # æ·»åŠ å›¾åƒä¿¡æ¯
              img = Image.open(img_file)
              coco_data["images"].append({
                  "id": image_id,
                  "file_name": img_file.name,
                  "width": img.size[0],
                  "height": img.size[1]
              })
              
              # å¤„ç†å¯¹åº”çš„æ ‡æ³¨æ–‡ä»¶
              annotation_file = annotations_dir / img_file.stem + ".txt"
              if annotation_file.exists():
                  with open(annotation_file, 'r') as f:
                      for line in f:
                          parts = line.strip().split(',')
                          if len(parts) >= 8:
                              # VisDroneæ ¼å¼è½¬æ¢
                              bbox_left = int(parts[0])
                              bbox_top = int(parts[1])
                              bbox_width = int(parts[2])
                              bbox_height = int(parts[3])
                              category_id = int(parts[5])
                              
                              # è½¬æ¢ä¸ºCOCOæ ¼å¼
                              coco_data["annotations"].append({
                                  "id": annotation_id,
                                  "image_id": image_id,
                                  "category_id": category_id,
                                  "bbox": [bbox_left, bbox_top, bbox_width, bbox_height],
                                  "area": bbox_width * bbox_height,
                                  "iscrowd": 0
                              })
                              annotation_id += 1
              
              image_id += 1
          
          return coco_data
  ```

**ä¸‹åˆ (14:00-18:00) - æ•°æ®éªŒè¯ä¸è´¨é‡æ£€æŸ¥**

**14:00-15:00 æ•°æ®éªŒè¯**
- [ ] **å®ç°æ•°æ®éªŒè¯è„šæœ¬**ï¼ˆ60åˆ†é’Ÿï¼‰
  ```python
  # data_validation.py
  import json
  from pathlib import Path
  import cv2
  import numpy as np
  from collections import Counter
  
  class DataValidator:
      def __init__(self, data_path="data/processed/visdrone"):
          self.data_path = Path(data_path)
          self.validation_report = {}
      
      def validate_dataset(self):
          """éªŒè¯æ•´ä¸ªæ•°æ®é›†"""
          print("å¼€å§‹éªŒè¯æ•°æ®é›†...")
          
          for split in ["train", "val", "test"]:
              split_path = self.data_path / split
              if split_path.exists():
                  print(f"éªŒè¯{split}é›†...")
                  self.validation_report[split] = self.validate_split(split_path)
          
          # ç”ŸæˆéªŒè¯æŠ¥å‘Š
          self.generate_validation_report()
          
          return self.validation_report
      
      def validate_split(self, split_path):
          """éªŒè¯ç‰¹å®šåˆ†å‰²çš„æ•°æ®"""
          images_dir = split_path / "images"
          annotations_file = split_path / "annotations" / "annotations.json"
          
          validation_result = {
              "total_images": 0,
              "total_annotations": 0,
              "image_issues": [],
              "annotation_issues": [],
              "class_distribution": {},
              "size_distribution": {}
          }
          
          # éªŒè¯å›¾åƒ
          if images_dir.exists():
              for img_file in images_dir.glob("*.jpg"):
                  validation_result["total_images"] += 1
                  
                  # æ£€æŸ¥å›¾åƒæ˜¯å¦å¯è¯»
                  try:
                      img = cv2.imread(str(img_file))
                      if img is None:
                          validation_result["image_issues"].append(f"æ— æ³•è¯»å–å›¾åƒ: {img_file.name}")
                      else:
                          # è®°å½•å›¾åƒå°ºå¯¸åˆ†å¸ƒ
                          h, w = img.shape[:2]
                          size_key = f"{w}x{h}"
                          validation_result["size_distribution"][size_key] = \
                              validation_result["size_distribution"].get(size_key, 0) + 1
                  except Exception as e:
                      validation_result["image_issues"].append(f"å›¾åƒå¤„ç†é”™è¯¯: {img_file.name} - {e}")
          
          # éªŒè¯æ ‡æ³¨
          if annotations_file.exists():
              with open(annotations_file, 'r') as f:
                  coco_data = json.load(f)
              
              validation_result["total_annotations"] = len(coco_data.get("annotations", []))
              
              # æ£€æŸ¥æ ‡æ³¨è´¨é‡
              for annotation in coco_data.get("annotations", []):
                  bbox = annotation.get("bbox", [])
                  if len(bbox) != 4:
                      validation_result["annotation_issues"].append(f"æ ‡æ³¨æ ¼å¼é”™è¯¯: {annotation['id']}")
                  elif bbox[2] <= 0 or bbox[3] <= 0:
                      validation_result["annotation_issues"].append(f"æ ‡æ³¨å°ºå¯¸é”™è¯¯: {annotation['id']}")
                  
                  # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
                  category_id = annotation.get("category_id", 0)
                  validation_result["class_distribution"][str(category_id)] = \
                      validation_result["class_distribution"].get(str(category_id), 0) + 1
          
          return validation_result
      
      def generate_validation_report(self):
          """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
          report_file = self.data_path / "validation_report.json"
          with open(report_file, 'w') as f:
              json.dump(self.validation_report, f, indent=2)
          
          print(f"éªŒè¯æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
          
          # æ‰“å°æ‘˜è¦
          print("\n=== æ•°æ®é›†éªŒè¯æ‘˜è¦ ===")
          for split, result in self.validation_report.items():
              print(f"\n{split}é›†:")
              print(f"  å›¾åƒæ•°é‡: {result['total_images']}")
              print(f"  æ ‡æ³¨æ•°é‡: {result['total_annotations']}")
              print(f"  å›¾åƒé—®é¢˜: {len(result['image_issues'])}")
              print(f"  æ ‡æ³¨é—®é¢˜: {len(result['annotation_issues'])}")
  ```

**15:00-15:30 ä¼‘æ¯**

**15:30-16:30 æ•°æ®å¢å¼ºå®ç°**
- [ ] **å®ç°æ•°æ®å¢å¼ºåŠŸèƒ½**ï¼ˆ60åˆ†é’Ÿï¼‰
  ```python
  # data_augmentation.py
  import cv2
  import numpy as np
  from PIL import Image
  import albumentations as A
  from albumentations.pytorch import ToTensorV2
  import json
  from pathlib import Path
  
  class DataAugmentation:
      def __init__(self):
          self.augmentation_pipeline = A.Compose([
              A.HorizontalFlip(p=0.5),
              A.Rotate(limit=15, p=0.5),
              A.RandomBrightnessContrast(
                  brightness_limit=0.2,
                  contrast_limit=0.2,
                  p=0.5
              ),
              A.Resize(height=640, width=640),
              A.Normalize(
                  mean=[0.485, 0.456, 0.406],
                  std=[0.229, 0.224, 0.225]
              ),
              ToTensorV2()
          ], bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']))
      
      def augment_dataset(self, input_dir, output_dir, num_augmentations=3):
          """å¢å¼ºæ•´ä¸ªæ•°æ®é›†"""
          input_path = Path(input_dir)
          output_path = Path(output_dir)
          output_path.mkdir(parents=True, exist_ok=True)
          
          # å¤„ç†æ¯ä¸ªåˆ†å‰²
          for split in ["train", "val", "test"]:
              split_input = input_path / split
              split_output = output_path / split
            
              if split_input.exists():
                  print(f"å¢å¼º{split}é›†...")
                  self.augment_split(split_input, split_output, num_augmentations)
      
      def augment_split(self, input_split, output_split, num_augmentations):
          """å¢å¼ºç‰¹å®šåˆ†å‰²çš„æ•°æ®"""
          output_split.mkdir(parents=True, exist_ok=True)
          
          # å¤åˆ¶åŸå§‹æ•°æ®
          import shutil
          shutil.copytree(input_split / "images", output_split / "images", dirs_exist_ok=True)
          shutil.copy2(input_split / "annotations" / "annotations.json", 
                      output_split / "annotations" / "annotations.json")
          
          # åŠ è½½åŸå§‹æ ‡æ³¨
          with open(input_split / "annotations" / "annotations.json", 'r') as f:
              coco_data = json.load(f)
          
          # ä¸ºæ¯å¼ å›¾åƒç”Ÿæˆå¢å¼ºç‰ˆæœ¬
          augmented_images = []
          augmented_annotations = []
          
          for image_info in coco_data["images"]:
              image_id = image_info["id"]
              image_file = input_split / "images" / image_info["file_name"]
              
              # è·å–è¯¥å›¾åƒçš„æ‰€æœ‰æ ‡æ³¨
              image_annotations = [ann for ann in coco_data["annotations"] if ann["image_id"] == image_id]
              
              # ç”Ÿæˆå¢å¼ºç‰ˆæœ¬
              for aug_idx in range(num_augmentations):
                  augmented_image, augmented_anns = self.augment_image(
                      image_file, image_annotations, aug_idx
                  )
                  
                  if augmented_image is not None:
                      # ä¿å­˜å¢å¼ºå›¾åƒ
                      aug_image_name = f"{image_info['file_name'].split('.')[0]}_aug_{aug_idx}.jpg"
                      aug_image_path = output_split / "images" / aug_image_name
                      cv2.imwrite(str(aug_image_path), augmented_image)
                      
                      # æ·»åŠ å›¾åƒä¿¡æ¯
                      new_image_id = len(coco_data["images"]) + len(augmented_images) + 1
                      augmented_images.append({
                          "id": new_image_id,
                          "file_name": aug_image_name,
                          "width": augmented_image.shape[1],
                          "height": augmented_image.shape[0]
                      })
                      
                      # æ·»åŠ æ ‡æ³¨ä¿¡æ¯
                      for ann in augmented_anns:
                          ann["id"] = len(coco_data["annotations"]) + len(augmented_annotations) + 1
                          ann["image_id"] = new_image_id
                          augmented_annotations.append(ann)
          
          # åˆå¹¶åŸå§‹æ•°æ®å’Œå¢å¼ºæ•°æ®
          final_coco_data = coco_data.copy()
          final_coco_data["images"].extend(augmented_images)
          final_coco_data["annotations"].extend(augmented_annotations)
          
          # ä¿å­˜æœ€ç»ˆæ ‡æ³¨æ–‡ä»¶
          with open(output_split / "annotations" / "annotations.json", 'w') as f:
              json.dump(final_coco_data, f, indent=2)
          
          print(f"å¢å¼ºå®Œæˆ: {len(augmented_images)} å¼ æ–°å›¾åƒ")
      
      def augment_image(self, image_path, annotations, aug_idx):
          """å¢å¼ºå•å¼ å›¾åƒ"""
          try:
              # è¯»å–å›¾åƒ
              image = cv2.imread(str(image_path))
              image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
              
              # å‡†å¤‡æ ‡æ³¨æ•°æ®
              bboxes = []
              category_ids = []
              
              for ann in annotations:
                  bbox = ann["bbox"]
                  # è½¬æ¢ä¸ºalbumentationsæ ¼å¼ [x_min, y_min, x_max, y_max]
                  x_min, y_min, w, h = bbox
                  x_max = x_min + w
                  y_max = y_min + h
                  bboxes.append([x_min, y_min, x_max, y_max])
                  category_ids.append(ann["category_id"])
              
              # åº”ç”¨å¢å¼º
              augmented = self.augmentation_pipeline(
                  image=image,
                  bboxes=bboxes,
                  category_ids=category_ids
              )
              
              augmented_image = augmented["image"]
              augmented_bboxes = augmented["bboxes"]
              augmented_category_ids = augmented["category_ids"]
              
              # è½¬æ¢å›COCOæ ¼å¼
              augmented_annotations = []
              for i, (bbox, cat_id) in enumerate(zip(augmented_bboxes, augmented_category_ids)):
                  x_min, y_min, x_max, y_max = bbox
                  w = x_max - x_min
                  h = y_max - y_min
                  
                  augmented_annotations.append({
                      "bbox": [x_min, y_min, w, h],
                      "area": w * h,
                      "category_id": cat_id,
                      "iscrowd": 0
                  })
              
              return augmented_image, augmented_annotations
              
          except Exception as e:
              print(f"å¢å¼ºå›¾åƒå¤±è´¥ {image_path}: {e}")
              return None, []
  ```

**16:30-17:30 æ•°æ®ç»Ÿè®¡ä¸åˆ†æ**
- [ ] **å®ç°æ•°æ®ç»Ÿè®¡åˆ†æ**ï¼ˆ60åˆ†é’Ÿï¼‰
  ```python
  # data_analysis.py
  import json
  import matplotlib.pyplot as plt
  import seaborn as sns
  from pathlib import Path
  import numpy as np
  from collections import Counter
  
  class DataAnalyzer:
      def __init__(self, data_path="data/processed/visdrone"):
          self.data_path = Path(data_path)
          self.class_names = [
              'ignored regions', 'pedestrian', 'people', 'bicycle', 'car', 
              'van', 'truck', 'tricycle', 'awning-tricycle', 'bus', 'motor'
          ]
      
      def analyze_dataset(self):
          """åˆ†ææ•´ä¸ªæ•°æ®é›†"""
          print("å¼€å§‹åˆ†ææ•°æ®é›†...")
          
          analysis_results = {}
          for split in ["train", "val", "test"]:
              split_path = self.data_path / split
              if split_path.exists():
                  print(f"åˆ†æ{split}é›†...")
                  analysis_results[split] = self.analyze_split(split_path)
          
          # ç”Ÿæˆåˆ†ææŠ¥å‘Š
          self.generate_analysis_report(analysis_results)
          
          return analysis_results
      
      def analyze_split(self, split_path):
          """åˆ†æç‰¹å®šåˆ†å‰²çš„æ•°æ®"""
          annotations_file = split_path / "annotations" / "annotations.json"
          
          if not annotations_file.exists():
              return None
          
          with open(annotations_file, 'r') as f:
              coco_data = json.load(f)
          
          analysis = {
              "total_images": len(coco_data["images"]),
              "total_annotations": len(coco_data["annotations"]),
              "class_distribution": {},
              "bbox_size_distribution": {},
              "image_size_distribution": {},
              "annotations_per_image": []
          }
          
          # åˆ†æç±»åˆ«åˆ†å¸ƒ
          category_counts = Counter()
          bbox_areas = []
          image_sizes = []
          annotations_per_image = {}
          
          for annotation in coco_data["annotations"]:
              category_id = annotation["category_id"]
              category_counts[category_id] += 1
              
              bbox = annotation["bbox"]
              area = bbox[2] * bbox[3]
              bbox_areas.append(area)
          
          for image_info in coco_data["images"]:
              width, height = image_info["width"], image_info["height"]
              image_sizes.append((width, height))
              
              # ç»Ÿè®¡æ¯å¼ å›¾åƒçš„æ ‡æ³¨æ•°é‡
              image_id = image_info["id"]
              ann_count = len([ann for ann in coco_data["annotations"] if ann["image_id"] == image_id])
              annotations_per_image[image_id] = ann_count
          
          # æ›´æ–°åˆ†æç»“æœ
          analysis["class_distribution"] = dict(category_counts)
          analysis["bbox_size_distribution"] = {
              "min": min(bbox_areas) if bbox_areas else 0,
              "max": max(bbox_areas) if bbox_areas else 0,
              "mean": np.mean(bbox_areas) if bbox_areas else 0,
              "std": np.std(bbox_areas) if bbox_areas else 0
          }
          analysis["annotations_per_image"] = list(annotations_per_image.values())
          
          return analysis
      
      def generate_analysis_report(self, analysis_results):
          """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
          # åˆ›å»ºå¯è§†åŒ–
          self.create_visualizations(analysis_results)
          
          # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
          report_file = self.data_path / "analysis_report.json"
          with open(report_file, 'w') as f:
              json.dump(analysis_results, f, indent=2)
          
          print(f"åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
      
      def create_visualizations(self, analysis_results):
          """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
          # è®¾ç½®ä¸­æ–‡å­—ä½“
          plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
          plt.rcParams['axes.unicode_minus'] = False
        
          # åˆ›å»ºå›¾è¡¨
          fig, axes = plt.subplots(2, 2, figsize=(15, 12))
          
          # 1. ç±»åˆ«åˆ†å¸ƒ
          all_class_counts = Counter()
          for split, result in analysis_results.items():
              if result and "class_distribution" in result:
                  all_class_counts.update(result["class_distribution"])
          
          if all_class_counts:
              class_names_short = [name[:10] for name in self.class_names]
              class_counts = [all_class_counts.get(i, 0) for i in range(len(self.class_names))]
              
              axes[0, 0].bar(range(len(class_names_short)), class_counts)
              axes[0, 0].set_title('ç±»åˆ«åˆ†å¸ƒ')
              axes[0, 0].set_xlabel('ç±»åˆ«')
              axes[0, 0].set_ylabel('æ ‡æ³¨æ•°é‡')
              axes[0, 0].set_xticks(range(len(class_names_short)))
              axes[0, 0].set_xticklabels(class_names_short, rotation=45)
          
          # 2. æ•°æ®é›†å¤§å°å¯¹æ¯”
          splits = list(analysis_results.keys())
          image_counts = [analysis_results[split]["total_images"] for split in splits if analysis_results[split]]
          annotation_counts = [analysis_results[split]["total_annotations"] for split in splits if analysis_results[split]]
          
          x = np.arange(len(splits))
          width = 0.35
          
          axes[0, 1].bar(x - width/2, image_counts, width, label='å›¾åƒæ•°é‡')
          axes[0, 1].bar(x + width/2, annotation_counts, width, label='æ ‡æ³¨æ•°é‡')
          axes[0, 1].set_title('æ•°æ®é›†å¤§å°å¯¹æ¯”')
          axes[0, 1].set_xlabel('æ•°æ®é›†åˆ†å‰²')
          axes[0, 1].set_ylabel('æ•°é‡')
          axes[0, 1].set_xticks(x)
          axes[0, 1].set_xticklabels(splits)
          axes[0, 1].legend()
          
          # 3. æ ‡æ³¨æ•°é‡åˆ†å¸ƒ
          all_annotations_per_image = []
          for split, result in analysis_results.items():
              if result and "annotations_per_image" in result:
                  all_annotations_per_image.extend(result["annotations_per_image"])
          
          if all_annotations_per_image:
              axes[1, 0].hist(all_annotations_per_image, bins=20, alpha=0.7)
              axes[1, 0].set_title('æ¯å¼ å›¾åƒæ ‡æ³¨æ•°é‡åˆ†å¸ƒ')
              axes[1, 0].set_xlabel('æ ‡æ³¨æ•°é‡')
              axes[1, 0].set_ylabel('å›¾åƒæ•°é‡')
          
          # 4. è¾¹ç•Œæ¡†å¤§å°åˆ†å¸ƒ
          axes[1, 1].text(0.5, 0.5, 'è¾¹ç•Œæ¡†å¤§å°åˆ†å¸ƒ\n(éœ€è¦é‡æ–°è®¡ç®—)', 
                         ha='center', va='center', transform=axes[1, 1].transAxes)
          axes[1, 1].set_title('è¾¹ç•Œæ¡†å¤§å°åˆ†å¸ƒ')
          
          plt.tight_layout()
          plt.savefig(self.data_path / "data_analysis.png", dpi=300, bbox_inches='tight')
          plt.show()
          
          print(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {self.data_path / 'data_analysis.png'}")
  ```

**17:30-18:00 æ•°æ®è´¨é‡æ£€æŸ¥**
- [ ] **æœ€ç»ˆæ•°æ®è´¨é‡æ£€æŸ¥**ï¼ˆ30åˆ†é’Ÿï¼‰
  - æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
  - éªŒè¯æ ‡æ³¨æ ¼å¼æ­£ç¡®æ€§
  - ç¡®è®¤æ•°æ®å¢å¼ºæ•ˆæœ
  - æ£€æŸ¥æ–‡ä»¶æƒé™å’Œè®¿é—®æ€§

---

## ğŸ“Š éªŒæ”¶æ ‡å‡†ä¸æ€»ç»“

### ç¬¬3å‘¨éªŒæ”¶æ ‡å‡†
- [ ] DVCç¯å¢ƒæ­£å¸¸è¿è¡Œï¼Œèƒ½å¤Ÿè¿›è¡Œæ•°æ®ç‰ˆæœ¬æ§åˆ¶
- [ ] åˆ†å±‚å­˜å‚¨æ¶æ„è®¾è®¡å®Œæˆï¼Œæ”¯æŒçƒ­/æ¸©/å†·å­˜å‚¨
- [ ] æ•°æ®æ ‡æ³¨æµç¨‹æ–‡æ¡£åŒ–ï¼Œæ”¯æŒå¤šç§æ ‡æ³¨å·¥å…·
- [ ] æ•°æ®å®‰å…¨æœºåˆ¶åŸºæœ¬å»ºç«‹ï¼ŒåŒ…æ‹¬åŠ å¯†ã€è®¿é—®æ§åˆ¶ã€å®¡è®¡

### ç¬¬4å‘¨éªŒæ”¶æ ‡å‡†
- [ ] å°è§„æ¨¡æ•°æ®é›†ï¼ˆVisDroneï¼‰æˆåŠŸä¸‹è½½å’Œé¢„å¤„ç†
- [ ] æ•°æ®å¢å¼ºåŠŸèƒ½æ­£å¸¸å·¥ä½œï¼Œèƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–è®­ç»ƒæ•°æ®
- [ ] æ•°æ®è´¨é‡éªŒè¯é€šè¿‡ï¼Œæ— ä¸¥é‡è´¨é‡é—®é¢˜
- [ ] å®Œæ•´çš„æ•°æ®ç®¡ç†æµç¨‹æµ‹è¯•æˆåŠŸ

### æŠ€æœ¯æˆæœ
1. **å®Œæ•´çš„æ•°æ®ç®¡ç†ç³»ç»Ÿ**ï¼šé›†æˆDVCã€MLflowã€å®‰å…¨æœºåˆ¶
2. **è‡ªåŠ¨åŒ–æ•°æ®å¤„ç†æµç¨‹**ï¼šä»åŸå§‹æ•°æ®åˆ°è®­ç»ƒå°±ç»ª
3. **æ•°æ®å®‰å…¨ä¸éšç§ä¿æŠ¤**ï¼šç¬¦åˆæ³•è§„è¦æ±‚çš„å®Œæ•´è§£å†³æ–¹æ¡ˆ
4. **è¯¦ç»†çš„æŠ€æœ¯æ–‡æ¡£**ï¼šä¾¿äºåç»­ç»´æŠ¤å’Œæ‰©å±•

### å­¦ä¹ æˆæœ
- æŒæ¡ç°ä»£æ•°æ®ç‰ˆæœ¬æ§åˆ¶æŠ€æœ¯
- ç†è§£ä¼ä¸šçº§æ•°æ®å®‰å…¨è¦æ±‚
- å­¦ä¼šè®¾è®¡å¯æ‰©å±•çš„æ•°æ®ç®¡ç†æ¶æ„
- å…·å¤‡å®Œæ•´çš„æ•°æ®ç§‘å­¦é¡¹ç›®ç»éªŒ

**æ­å–œå®Œæˆç¬¬3-4å‘¨çš„æ•°æ®ç®¡ç†ç³»ç»Ÿå»ºè®¾ä»»åŠ¡ï¼** ğŸ‰

### ç¬¬4å‘¨ï¼šå®Œæ•´è®­ç»ƒæµç¨‹æµ‹è¯•

#### å‘¨ä¸€ï¼šå°è§„æ¨¡æ•°æ®é›†å‡†å¤‡

**ä¸Šåˆ (9:00-12:00)**
- [ ] ä¸‹è½½VisDroneæ•°æ®é›†
  ```bash
  # åˆ›å»ºæ•°æ®é›†ç›®å½•
  mkdir -p data/external/visdrone
  cd data/external/visdrone
  
  # ä¸‹è½½æ•°æ®é›†ï¼ˆéœ€è¦æ³¨å†Œï¼‰
  wget https://github.com/VisDrone/VisDrone-Dataset.git
  ```

**ä¸‹åˆ (14:00-18:00)**
- [ ] æ•°æ®é¢„å¤„ç†è„šæœ¬
  ```python
  # data_preprocessing.py
  import os
  import json
  from pathlib import Path
  
  class VisDroneProcessor:
      def __init__(self, data_path):
          self.data_path = Path(data_path)
          self.output_path = Path("data/processed")
      
      def convert_annotations(self):
          """è½¬æ¢æ ‡æ³¨æ ¼å¼"""
          # ä»VisDroneæ ¼å¼è½¬æ¢ä¸ºCOCOæ ¼å¼
          pass
      
      def split_dataset(self, train_ratio=0.7, val_ratio=0.2):
          """åˆ†å‰²æ•°æ®é›†"""
          # æŒ‰æ¯”ä¾‹åˆ†å‰²è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†
          pass
  ```

#### å‘¨äºŒï¼šDVCæ•°æ®ç‰ˆæœ¬ç®¡ç†æµ‹è¯•

**ä¸Šåˆ (9:00-12:00)**
- [ ] æ·»åŠ æ•°æ®åˆ°DVC
  ```bash
  # æ·»åŠ åŸå§‹æ•°æ®
  dvc add data/raw/images
  dvc add data/raw/annotations
  
  # æ·»åŠ å¤„ç†åæ•°æ®
  dvc add data/processed/train
  dvc add data/processed/val
  dvc add data/processed/test
  
  # æäº¤åˆ°Git
  git add data.dvc .dvc
  git commit -m "Add drone dataset v1.0"
  ```

**ä¸‹åˆ (14:00-18:00)**
- [ ] æµ‹è¯•æ•°æ®ç‰ˆæœ¬åˆ‡æ¢
  ```bash
  # åˆ‡æ¢åˆ°ä¸åŒç‰ˆæœ¬
  git checkout v1.0
  dvc checkout
  
  # åˆ›å»ºæ–°ç‰ˆæœ¬
  dvc add data/processed/train_v2
  git add data.dvc
  git commit -m "Update dataset to v1.1"
  ```

#### å‘¨ä¸‰ï¼šMLflowå®éªŒè·Ÿè¸ªé›†æˆ

**ä¸Šåˆ (9:00-12:00)**
- [ ] é…ç½®MLflowä¸DVCé›†æˆ
  ```python
  # mlflow_dvc_integration.py
  import mlflow
  import dvc.api
  from pathlib import Path
  
  class MLflowDVCIntegration:
      def __init__(self):
          mlflow.set_experiment("drone_vision_experiment")
      
      def log_data_version(self, data_path):
          """è®°å½•æ•°æ®ç‰ˆæœ¬åˆ°MLflow"""
          with mlflow.start_run():
              # è·å–DVCæ•°æ®ç‰ˆæœ¬
              data_version = dvc.api.get_url(data_path)
              mlflow.log_param("data_version", data_version)
              mlflow.log_param("data_path", str(data_path))
  ```

**ä¸‹åˆ (14:00-18:00)**
- [ ] å®ç°æ•°æ®è¡€ç¼˜è¿½è¸ª
  ```python
  # data_lineage.py
  class DataLineage:
      def __init__(self):
          self.lineage_graph = {}
      
      def track_data_flow(self, source, target, transformation):
          """è¿½è¸ªæ•°æ®æµè½¬"""
          if source not in self.lineage_graph:
              self.lineage_graph[source] = []
          self.lineage_graph[source].append({
              'target': target,
              'transformation': transformation,
              'timestamp': datetime.now()
          })
  ```

#### å‘¨å››ï¼šå®Œæ•´è®­ç»ƒæµç¨‹æµ‹è¯•

**ä¸Šåˆ (9:00-12:00)**
- [ ] åˆ›å»ºè®­ç»ƒè„šæœ¬
  ```python
  # train_with_dvc_mlflow.py
  import mlflow
  import dvc.api
  from main import DroneVisionExperiment
  
  def main():
      # ä»DVCè·å–æ•°æ®
      data_path = dvc.api.get_url("data/processed/train")
      
      # å¯åŠ¨MLflowå®éªŒ
      with mlflow.start_run():
          # è®°å½•æ•°æ®ç‰ˆæœ¬
          mlflow.log_param("data_version", data_path)
          
          # åˆ›å»ºå®éªŒ
          experiment = DroneVisionExperiment("DVC_MLflow_Integration")
          
          # è®­ç»ƒæ¨¡å‹
          history, accuracy = experiment.train_model()
          
          # è®°å½•ç»“æœ
          mlflow.log_metric("final_accuracy", accuracy)
  ```

**ä¸‹åˆ (14:00-18:00)**
- [ ] è¿è¡Œå®Œæ•´æµç¨‹
  ```bash
  # è¿è¡Œè®­ç»ƒ
  python train_with_dvc_mlflow.py
  
  # æ£€æŸ¥ç»“æœ
  mlflow ui  # æŸ¥çœ‹å®éªŒè®°å½•
  dvc metrics show  # æŸ¥çœ‹æ•°æ®æŒ‡æ ‡
  ```

#### å‘¨äº”ï¼šæ€§èƒ½ä¼˜åŒ–ä¸æ–‡æ¡£

**ä¸Šåˆ (9:00-12:00)**
- [ ] æ€§èƒ½ç›‘æ§
  ```python
  # performance_monitor.py
  import psutil
  import time
  from memory_profiler import profile
  
  class PerformanceMonitor:
      def __init__(self):
          self.metrics = {}
      
      @profile
      def monitor_training(self, training_func):
          """ç›‘æ§è®­ç»ƒæ€§èƒ½"""
          start_time = time.time()
          start_memory = psutil.virtual_memory().used
          
          result = training_func()
          
          end_time = time.time()
          end_memory = psutil.virtual_memory().used
          
          self.metrics = {
              'training_time': end_time - start_time,
              'memory_usage': end_memory - start_memory,
              'cpu_usage': psutil.cpu_percent()
          }
          
          return result
  ```

**ä¸‹åˆ (14:00-18:00)**
- [ ] ç¼–å†™æŠ€æœ¯æ–‡æ¡£
  ```markdown
  # æ•°æ®ç®¡ç†ç³»ç»ŸæŠ€æœ¯æ–‡æ¡£
  
  ## ç³»ç»Ÿæ¶æ„
  - DVCæ•°æ®ç‰ˆæœ¬ç®¡ç†
  - åˆ†å±‚å­˜å‚¨æ¶æ„
  - æ•°æ®å®‰å…¨æœºåˆ¶
  
  ## ä½¿ç”¨æŒ‡å—
  - æ•°æ®æ·»åŠ æµç¨‹
  - ç‰ˆæœ¬ç®¡ç†æ“ä½œ
  - å®‰å…¨è®¿é—®æ§åˆ¶
  ```

---

## ğŸ› ï¸ æŠ€æœ¯æ ˆä¸å·¥å…·

### æ•°æ®ç‰ˆæœ¬ç®¡ç†
- **DVC**: æ•°æ®ç‰ˆæœ¬æ§åˆ¶
- **Git**: ä»£ç ç‰ˆæœ¬æ§åˆ¶
- **MLflow**: å®éªŒè·Ÿè¸ª

### å­˜å‚¨æ–¹æ¡ˆ
- **æœ¬åœ°å­˜å‚¨**: SSD + HDD
- **äº‘å­˜å‚¨**: AWS S3 / é˜¿é‡Œäº‘OSS
- **ç¼“å­˜**: Redis

### å®‰å…¨å·¥å…·
- **åŠ å¯†**: cryptography
- **è®¿é—®æ§åˆ¶**: è‡ªå®šä¹‰æƒé™ç³»ç»Ÿ
- **å®¡è®¡**: æ—¥å¿—è®°å½•

### æ•°æ®å¤„ç†
- **æ ¼å¼è½¬æ¢**: OpenCV, PIL
- **æ•°æ®å¢å¼º**: albumentations
- **æ ‡æ³¨å·¥å…·**: labelme, CVAT

---

## ğŸ“Š éªŒæ”¶æ ‡å‡†

### ç¬¬3å‘¨éªŒæ”¶
- [ ] DVCç¯å¢ƒæ­£å¸¸è¿è¡Œ
- [ ] æ•°æ®å­˜å‚¨æ¶æ„è®¾è®¡å®Œæˆ
- [ ] æ ‡æ³¨æµç¨‹æ–‡æ¡£åŒ–
- [ ] å®‰å…¨æœºåˆ¶åŸºæœ¬å»ºç«‹

### ç¬¬4å‘¨éªŒæ”¶
- [ ] å°è§„æ¨¡æ•°æ®é›†è®­ç»ƒæˆåŠŸ
- [ ] DVCä¸MLflowé›†æˆæ­£å¸¸
- [ ] æ•°æ®è¡€ç¼˜è¿½è¸ªå¯ç”¨
- [ ] å®Œæ•´æµç¨‹æ–‡æ¡£å®Œæˆ

---

## ğŸš¨ é£é™©ä¸åº”å¯¹

### é£é™©1: æ•°æ®é‡è¿‡å¤§å¯¼è‡´å­˜å‚¨ä¸è¶³
**åº”å¯¹**: å®æ–½åˆ†å±‚å­˜å‚¨ï¼ŒåŠæ—¶å½’æ¡£å†·æ•°æ®

### é£é™©2: æ•°æ®å®‰å…¨é—®é¢˜
**åº”å¯¹**: å¤šå±‚åŠ å¯†ï¼Œè®¿é—®æ§åˆ¶ï¼Œå®¡è®¡æ—¥å¿—

### é£é™©3: ç‰ˆæœ¬ç®¡ç†æ··ä¹±
**åº”å¯¹**: ä¸¥æ ¼çš„å‘½åè§„èŒƒï¼Œè‡ªåŠ¨åŒ–ç‰ˆæœ¬æ§åˆ¶

### é£é™©4: è®­ç»ƒæµç¨‹ä¸­æ–­
**åº”å¯¹**: æ–­ç‚¹ç»­è®­ï¼Œé”™è¯¯æ¢å¤æœºåˆ¶

---

## ğŸ“ æ”¯æŒèµ„æº

### æŠ€æœ¯æ–‡æ¡£
- [DVCå®˜æ–¹æ–‡æ¡£](https://dvc.org/doc)
- [MLflowå®˜æ–¹æ–‡æ¡£](https://mlflow.org/docs/)
- [VisDroneæ•°æ®é›†](https://github.com/VisDrone/VisDrone-Dataset)

### ç¤¾åŒºæ”¯æŒ
- DVC Slackç¤¾åŒº
- MLflow GitHub Issues
- å†…éƒ¨æŠ€æœ¯äº¤æµç¾¤

---

**ç¥æ‚¨ç¬¬3-4å‘¨å·¥ä½œé¡ºåˆ©ï¼** ğŸšâœ¨
