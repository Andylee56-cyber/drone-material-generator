# ============================================
# æ›´æ–°8ç»´è¯„åˆ†æ ‡å‡†å’Œå¢å¼ºè®­ç»ƒç›®æ ‡ï¼ˆé€‚é…VisDroneæ•°æ®é›†ï¼‰
# ============================================

Write-Host "å¼€å§‹æ›´æ–°è¯„åˆ†æ ‡å‡†å’Œå¢å¼ºè®­ç»ƒç³»ç»Ÿ..." -ForegroundColor Cyan

# ============================================
# æ­¥éª¤1: æ›´æ–°8ç»´è¯„åˆ†æ ‡å‡†ï¼ˆé€‚é…VisDroneæ•°æ®é›†ï¼‰
# ============================================
Write-Host "`næ­¥éª¤1: æ›´æ–°8ç»´è¯„åˆ†æ ‡å‡†..." -ForegroundColor Yellow

@"
"""
8ç»´åº¦å›¾ç‰‡è´¨é‡åˆ†æAgentï¼ˆé€‚é…VisDroneæ•°æ®é›†ï¼‰
8-Dimensional Image Quality Analysis Agent (VisDrone Optimized)
ç”¨äºåˆ†ææ— äººæœºå›¾ç‰‡ç´ æçš„8ä¸ªå…³é”®ç»´åº¦ï¼Œè¯„åˆ†æ ‡å‡†å·²è°ƒæ•´ä¸ºé€‚é…VisDroneæ•°æ®é›†
"""

import numpy as np
import cv2
from PIL import Image
import torch
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from ultralytics import YOLO
import json
from datetime import datetime


class ImageQualityAnalyzer:
    """8ç»´åº¦å›¾ç‰‡è´¨é‡åˆ†æå™¨ï¼ˆVisDroneä¼˜åŒ–ç‰ˆï¼‰"""
    
    def __init__(self, yolo_model_path: Optional[str] = None):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        å‚æ•°:
            yolo_model_path: YOLOæ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
        """
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åŠ è½½YOLOæ¨¡å‹ç”¨äºç›®æ ‡æ£€æµ‹
        if yolo_model_path and Path(yolo_model_path).exists():
            self.detector = YOLO(yolo_model_path)
        else:
            # ä½¿ç”¨é¢„è®­ç»ƒçš„YOLOv8næ¨¡å‹
            self.detector = YOLO('yolov8n.pt')
        
        # 8ä¸ªç»´åº¦çš„åç§°
        self.dimensions = [
            "å›¾ç‰‡æ•°æ®é‡",
            "æ‹æ‘„å…‰ç…§è´¨é‡",
            "ç›®æ ‡å°ºå¯¸",
            "ç›®æ ‡å®Œæ•´æ€§",
            "æ•°æ®å‡è¡¡åº¦",
            "äº§å“ä¸°å¯Œåº¦",
            "ç›®æ ‡å¯†é›†åº¦",
            "åœºæ™¯å¤æ‚åº¦"
        ]
        
    def analyze_single_image(self, image_path: str) -> Dict:
        """
        åˆ†æå•å¼ å›¾ç‰‡çš„8ä¸ªç»´åº¦
        
        å‚æ•°:
            image_path: å›¾ç‰‡è·¯å¾„
            
        è¿”å›:
            åŒ…å«8ä¸ªç»´åº¦åˆ†æ•°çš„å­—å…¸
        """
        # è¯»å–å›¾ç‰‡
        img = cv2.imread(image_path)
        if img is None:
            raise ValueError(f"æ— æ³•è¯»å–å›¾ç‰‡: {image_path}")
        
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        h, w = img.shape[:2]
        
        # 1. å›¾ç‰‡æ•°æ®é‡ (åŸºäºå›¾ç‰‡åˆ†è¾¨ç‡å’Œæ–‡ä»¶å¤§å°)
        data_quantity = self._calculate_data_quantity(image_path, h, w)
        
        # 2. æ‹æ‘„å…‰ç…§è´¨é‡ (åŸºäºäº®åº¦ã€å¯¹æ¯”åº¦ã€ç›´æ–¹å›¾åˆ†æ)
        lighting_quality = self._calculate_lighting_quality(img_rgb)
        
        # 3. ç›®æ ‡å°ºå¯¸ (åŸºäºæ£€æµ‹åˆ°çš„ç›®æ ‡å¹³å‡å°ºå¯¸)
        target_size = self._calculate_target_size(img_rgb)
        
        # 4. ç›®æ ‡å®Œæ•´æ€§ (åŸºäºç›®æ ‡æ˜¯å¦è¢«è£å‰ªæˆ–é®æŒ¡)
        target_completeness = self._calculate_target_completeness(img_rgb)
        
        # 5. æ•°æ®å‡è¡¡åº¦ (åŸºäºä¸åŒç±»åˆ«ç›®æ ‡çš„åˆ†å¸ƒ)
        data_balance = self._calculate_data_balance(img_rgb)
        
        # 6. äº§å“ä¸°å¯Œåº¦ (åŸºäºæ£€æµ‹åˆ°çš„ç›®æ ‡ç±»åˆ«æ•°é‡)
        product_richness = self._calculate_product_richness(img_rgb)
        
        # 7. ç›®æ ‡å¯†é›†åº¦ (åŸºäºå•ä½é¢ç§¯å†…çš„ç›®æ ‡æ•°é‡)
        target_density = self._calculate_target_density(img_rgb, h, w)
        
        # 8. åœºæ™¯å¤æ‚åº¦ (åŸºäºèƒŒæ™¯å¤æ‚åº¦ã€çº¹ç†ä¸°å¯Œåº¦)
        scene_complexity = self._calculate_scene_complexity(img_rgb)
        
        return {
            "å›¾ç‰‡æ•°æ®é‡": data_quantity,
            "æ‹æ‘„å…‰ç…§è´¨é‡": lighting_quality,
            "ç›®æ ‡å°ºå¯¸": target_size,
            "ç›®æ ‡å®Œæ•´æ€§": target_completeness,
            "æ•°æ®å‡è¡¡åº¦": data_balance,
            "äº§å“ä¸°å¯Œåº¦": product_richness,
            "ç›®æ ‡å¯†é›†åº¦": target_density,
            "åœºæ™¯å¤æ‚åº¦": scene_complexity
        }
    
    def analyze_batch(self, image_paths: List[str]) -> Dict:
        """
        æ‰¹é‡åˆ†æå¤šå¼ å›¾ç‰‡
        
        å‚æ•°:
            image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
            
        è¿”å›:
            åŒ…å«æ‰€æœ‰å›¾ç‰‡åˆ†æç»“æœçš„å­—å…¸
        """
        results = []
        for img_path in image_paths:
            try:
                result = self.analyze_single_image(img_path)
                result['image_path'] = img_path
                results.append(result)
            except Exception as e:
                print(f"åˆ†æå›¾ç‰‡ {img_path} æ—¶å‡ºé”™: {e}")
                continue
        
        # è®¡ç®—å¹³å‡ç»´åº¦åˆ†æ•°
        avg_scores = {}
        for dim in self.dimensions:
            scores = [r[dim] for r in results if dim in r]
            avg_scores[dim] = np.mean(scores) if scores else 0.0
        
        return {
            "individual_results": results,
            "average_scores": avg_scores,
            "total_images": len(results),
            "total_annotations": sum(len(self._detect_objects(r['image_path'])) for r in results)
        }
    
    def _calculate_data_quantity(self, image_path: str, height: int, width: int) -> float:
        """è®¡ç®—å›¾ç‰‡æ•°æ®é‡ç»´åº¦ (0-100) - VisDroneä¼˜åŒ–ï¼šé™ä½æ ‡å‡†"""
        # åŸºäºåˆ†è¾¨ç‡å’Œæ–‡ä»¶å¤§å°
        file_size = Path(image_path).stat().st_size / (1024 * 1024)  # MB
        pixel_count = height * width
        
        # å½’ä¸€åŒ–åˆ°0-100
        # VisDroneä¼˜åŒ–ï¼šç†æƒ³å€¼é™ä½ä¸º åˆ†è¾¨ç‡ >= 1280x720, æ–‡ä»¶å¤§å° >= 1MB
        resolution_score = min(100, (pixel_count / (1280 * 720)) * 100)
        size_score = min(100, (file_size / 1.0) * 100)
        
        # å¦‚æœè¾¾åˆ°æœ€ä½æ ‡å‡†ï¼ˆ640x480, 0.5MBï¼‰ï¼Œè‡³å°‘ç»™30åˆ†
        if pixel_count >= 640 * 480 and file_size >= 0.5:
            resolution_score = max(30, resolution_score)
            size_score = max(30, size_score)
        
        return (resolution_score * 0.6 + size_score * 0.4)
    
    def _calculate_lighting_quality(self, img: np.ndarray) -> float:
        """è®¡ç®—æ‹æ‘„å…‰ç…§è´¨é‡ç»´åº¦ (0-100) - VisDroneä¼˜åŒ–ï¼šæ”¾å®½æ ‡å‡†"""
        # è½¬æ¢ä¸ºHSVè‰²å½©ç©ºé—´
        hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)
        v_channel = hsv[:, :, 2]  # äº®åº¦é€šé“
        
        # è®¡ç®—äº®åº¦ç»Ÿè®¡
        mean_brightness = np.mean(v_channel)
        std_brightness = np.std(v_channel)
        
        # VisDroneä¼˜åŒ–ï¼šç†æƒ³äº®åº¦èŒƒå›´æ”¾å®½ä¸º 80-220 (0-255èŒƒå›´)
        brightness_score = 100 - abs(mean_brightness - 150) / 150 * 100
        brightness_score = max(0, min(100, brightness_score))
        
        # å¯¹æ¯”åº¦è¯„åˆ† (æ ‡å‡†å·®è¶Šå¤§ï¼Œå¯¹æ¯”åº¦è¶Šå¥½) - é™ä½è¦æ±‚
        contrast_score = min(100, std_brightness / 2.0)  # ä»2.55é™åˆ°2.0
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¿‡æ›æˆ–æ¬ æ› - å‡å°‘æƒ©ç½š
        overexposed = np.sum(v_channel > 240) / v_channel.size
        underexposed = np.sum(v_channel < 15) / v_channel.size
        exposure_penalty = (overexposed + underexposed) * 30  # ä»50é™åˆ°30
        
        final_score = (brightness_score * 0.4 + contrast_score * 0.4) - exposure_penalty
        # æœ€ä½ä¿è¯20åˆ†ï¼ˆVisDroneæ•°æ®é›†é€šå¸¸å…‰ç…§ä¸ç†æƒ³ï¼‰
        return max(20, min(100, final_score))
    
    def _detect_objects(self, img: np.ndarray) -> List[Dict]:
        """ä½¿ç”¨YOLOæ£€æµ‹ç›®æ ‡"""
        try:
            results = self.detector(img, verbose=False)
            detections = []
            for result in results:
                boxes = result.boxes
                for box in boxes:
                    detections.append({
                        'class': int(box.cls[0]),
                        'confidence': float(box.conf[0]),
                        'bbox': box.xyxy[0].cpu().numpy().tolist()  # [x1, y1, x2, y2]
                    })
            return detections
        except Exception as e:
            print(f"ç›®æ ‡æ£€æµ‹å‡ºé”™: {e}")
            return []
    
    def _calculate_target_size(self, img: np.ndarray) -> float:
        """è®¡ç®—ç›®æ ‡å°ºå¯¸ç»´åº¦ (0-100) - VisDroneä¼˜åŒ–ï¼šé™ä½ç†æƒ³å æ¯”"""
        detections = self._detect_objects(img)
        if not detections:
            return 0.0
        
        h, w = img.shape[:2]
        total_area = h * w
        
        # è®¡ç®—æ‰€æœ‰æ£€æµ‹æ¡†çš„å¹³å‡é¢ç§¯å æ¯”
        area_ratios = []
        for det in detections:
            x1, y1, x2, y2 = det['bbox']
            box_area = (x2 - x1) * (y2 - y1)
            area_ratio = box_area / total_area
            area_ratios.append(area_ratio)
        
        avg_ratio = np.mean(area_ratios) if area_ratios else 0
        
        # VisDroneä¼˜åŒ–ï¼šç†æƒ³ç›®æ ‡å°ºå¯¸å æ¯”é™ä½ä¸º 2-10%ï¼ˆä»5-15%é™ä½ï¼‰
        if 0.02 <= avg_ratio <= 0.10:
            return 100.0
        elif avg_ratio < 0.02:
            return (avg_ratio / 0.02) * 100
        else:
            return max(0, 100 - ((avg_ratio - 0.10) / 0.10) * 100)
    
    def _calculate_target_completeness(self, img: np.ndarray) -> float:
        """è®¡ç®—ç›®æ ‡å®Œæ•´æ€§ç»´åº¦ (0-100) - VisDroneä¼˜åŒ–ï¼šå‡å°‘è¾¹ç¼˜æƒ©ç½š"""
        detections = self._detect_objects(img)
        if not detections:
            return 0.0
        
        h, w = img.shape[:2]
        completeness_scores = []
        
        for det in detections:
            x1, y1, x2, y2 = det['bbox']
            
            # æ£€æŸ¥ç›®æ ‡æ˜¯å¦é è¿‘è¾¹ç¼˜ (å¯èƒ½è¢«è£å‰ª) - VisDroneä¼˜åŒ–ï¼šæ”¾å®½è¾¹è·
            margin = 0.03  # ä»5%é™åˆ°3%
            near_left = x1 < w * margin
            near_right = x2 > w * (1 - margin)
            near_top = y1 < h * margin
            near_bottom = y2 > h * (1 - margin)
            
            # å¦‚æœç›®æ ‡åœ¨è¾¹ç¼˜ï¼Œå¯èƒ½ä¸å®Œæ•´ - å‡å°‘æƒ©ç½š
            edge_penalty = (near_left + near_right + near_top + near_bottom) * 5  # ä»10é™åˆ°5
            
            # æ£€æŸ¥ç½®ä¿¡åº¦ (ä½ç½®ä¿¡åº¦å¯èƒ½è¡¨ç¤ºç›®æ ‡ä¸å®Œæ•´)
            confidence_score = det['confidence'] * 100
            
            completeness = max(0, confidence_score - edge_penalty)
            completeness_scores.append(completeness)
        
        result = np.mean(completeness_scores) if completeness_scores else 0.0
        # æœ€ä½ä¿è¯15åˆ†
        return max(15, result)
    
    def _calculate_data_balance(self, img: np.ndarray) -> float:
        """è®¡ç®—æ•°æ®å‡è¡¡åº¦ç»´åº¦ (0-100) - VisDroneä¼˜åŒ–ï¼šä¿æŒä½†æ”¾å®½"""
        detections = self._detect_objects(img)
        if not detections:
            return 0.0
        
        # ç»Ÿè®¡å„ç±»åˆ«çš„æ•°é‡
        class_counts = {}
        for det in detections:
            cls = det['class']
            class_counts[cls] = class_counts.get(cls, 0) + 1
        
        if len(class_counts) == 0:
            return 0.0
        
        # è®¡ç®—ç±»åˆ«åˆ†å¸ƒçš„å‡è¡¡åº¦ (ä½¿ç”¨ç†µ)
        total = sum(class_counts.values())
        probs = [count / total for count in class_counts.values()]
        entropy = -sum(p * np.log2(p + 1e-10) for p in probs)
        max_entropy = np.log2(len(class_counts))
        
        # å½’ä¸€åŒ–åˆ°0-100
        balance_score = (entropy / max_entropy) * 100 if max_entropy > 0 else 0
        
        # æœ€ä½ä¿è¯20åˆ†ï¼ˆå³ä½¿ä¸å‡è¡¡ä¹Ÿæœ‰åŸºç¡€åˆ†ï¼‰
        return max(20, balance_score)
    
    def _calculate_product_richness(self, img: np.ndarray) -> float:
        """è®¡ç®—äº§å“ä¸°å¯Œåº¦ç»´åº¦ (0-100) - VisDroneä¼˜åŒ–ï¼šé™ä½ç†æƒ³ç±»åˆ«æ•°"""
        detections = self._detect_objects(img)
        unique_classes = len(set(det['class'] for det in detections))
        
        # VisDroneä¼˜åŒ–ï¼šç†æƒ³æƒ…å†µé™ä½ä¸º 3-6ä¸ªä¸åŒç±»åˆ«ï¼ˆä»5-10é™ä½ï¼‰
        if unique_classes == 0:
            return 0.0
        elif unique_classes <= 6:
            return (unique_classes / 6) * 100
        else:
            # è¶…è¿‡6ä¸ªç±»åˆ«ï¼Œç»™äºˆé¢å¤–å¥–åŠ±ä½†ä¸è¶…è¿‡100
            return min(100, 100 + (unique_classes - 6) * 3)
    
    def _calculate_target_density(self, img: np.ndarray, height: int, width: int) -> float:
        """è®¡ç®—ç›®æ ‡å¯†é›†åº¦ç»´åº¦ (0-100) - VisDroneä¼˜åŒ–ï¼šé™ä½ç†æƒ³å¯†é›†åº¦"""
        detections = self._detect_objects(img)
        num_targets = len(detections)
        
        if num_targets == 0:
            return 0.0
        
        # è®¡ç®—å•ä½é¢ç§¯å†…çš„ç›®æ ‡æ•°é‡
        area = height * width / (1000 * 1000)  # è½¬æ¢ä¸ºç™¾ä¸‡åƒç´ 
        density = num_targets / (area + 1e-6)
        
        # VisDroneä¼˜åŒ–ï¼šç†æƒ³å¯†é›†åº¦é™ä½ä¸º æ¯ç™¾ä¸‡åƒç´ 2-8ä¸ªç›®æ ‡ï¼ˆä»5-15é™ä½ï¼‰
        if 2 <= density <= 8:
            return 100.0
        elif density < 2:
            return (density / 2) * 100
        else:
            # è¿‡äºå¯†é›†å¯èƒ½å½±å“è´¨é‡
            return max(0, 100 - ((density - 8) / 8) * 50)
    
    def _calculate_scene_complexity(self, img: np.ndarray) -> float:
        """è®¡ç®—åœºæ™¯å¤æ‚åº¦ç»´åº¦ (0-100) - VisDroneä¼˜åŒ–ï¼šç¨å¾®æ”¾å®½"""
        # è½¬æ¢ä¸ºç°åº¦å›¾
        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        
        # ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯ç®—å­è®¡ç®—å›¾åƒæ¸…æ™°åº¦/å¤æ‚åº¦
        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
        
        # è®¡ç®—çº¹ç†å¤æ‚åº¦ (ä½¿ç”¨å±€éƒ¨äºŒå€¼æ¨¡å¼æˆ–è¾¹ç¼˜æ£€æµ‹)
        edges = cv2.Canny(gray, 50, 150)
        edge_density = np.sum(edges > 0) / edges.size
        
        # è®¡ç®—é¢œè‰²å¤æ‚åº¦
        unique_colors = len(np.unique(img.reshape(-1, img.shape[-1]), axis=0))
        color_complexity = min(100, (unique_colors / 800) * 100)  # ä»1000é™åˆ°800
        
        # ç»¼åˆè¯„åˆ†
        sharpness_score = min(100, (laplacian_var / 400) * 100)  # ä»500é™åˆ°400
        texture_score = min(100, edge_density * 1200)  # ä»1000æé«˜åˆ°1200
        
        complexity = (sharpness_score * 0.3 + texture_score * 0.3 + color_complexity * 0.4)
        # æœ€ä½ä¿è¯25åˆ†ï¼ˆVisDroneé€šå¸¸æœ‰ä¸€å®šå¤æ‚åº¦ï¼‰
        return max(25, min(100, complexity))


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    analyzer = ImageQualityAnalyzer()
    test_image = "test_image.jpg"  # æ›¿æ¢ä¸ºå®é™…å›¾ç‰‡è·¯å¾„
    
    if Path(test_image).exists():
        result = analyzer.analyze_single_image(test_image)
        print("8ç»´åº¦åˆ†æç»“æœ:")
        for dim, score in result.items():
            print(f"{dim}: {score:.2f}%")
"@ | Set-Content "agents\image_quality_analyzer.py" -Encoding UTF8

Write-Host "âœ… æ­¥éª¤1å®Œæˆï¼šå·²æ›´æ–°8ç»´è¯„åˆ†æ ‡å‡†ï¼ˆé€‚é…VisDroneæ•°æ®é›†ï¼‰" -ForegroundColor Green

# ============================================
# æ­¥éª¤2: æ›´æ–°å¢å¼ºè®­ç»ƒå™¨ï¼ˆæ”¹ä¸ºåŸºäºæå‡å¹…åº¦ï¼‰
# ============================================
Write-Host "`næ­¥éª¤2: æ›´æ–°å¢å¼ºè®­ç»ƒå™¨ï¼ˆæ”¹ä¸ºåŸºäºæå‡å¹…åº¦ï¼‰..." -ForegroundColor Yellow

@"
"""
GPUåŠ é€Ÿç‰ˆç´ æè‡ªåŠ¨å¢å¼ºè®­ç»ƒå™¨ï¼ˆåŸºäºæå‡å¹…åº¦è¯„ä¼°ï¼‰
"""
import numpy as np
import cv2
from PIL import Image
from pathlib import Path
from typing import Dict, List, Optional
from agents.image_quality_analyzer import ImageQualityAnalyzer
from agents.material_generator_agent import MaterialGeneratorAgent
import torch
import torch.nn.functional as F

class MaterialEnhancementTrainer:
    """æ”¯æŒGPUå¹¶è¡Œçš„ç´ æè‡ªåŠ¨å¢å¼ºè®­ç»ƒå™¨ï¼ˆåŸºäºæå‡å¹…åº¦è¯„ä¼°ï¼‰"""

    def __init__(self, yolo_model_path: Optional[str] = None,
                 fast_mode: bool = True, analysis_max_side: int = 960):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.analyzer = ImageQualityAnalyzer(yolo_model_path)
        self.agent = MaterialGeneratorAgent(yolo_model_path)
        # æ”¹ä¸ºåŸºäºæå‡å¹…åº¦çš„ç›®æ ‡
        self.target_improvement = 5.0  # ç›®æ ‡æå‡5åˆ†ï¼ˆä¼˜ç§€ï¼‰
        self.excellent_threshold = 8.0  # æå‡8åˆ†ä»¥ä¸Šä¸ºä¼˜ç§€
        self.good_threshold = 5.0  # æå‡5-8åˆ†ä¸ºè‰¯å¥½
        self.fair_threshold = 3.0  # æå‡3-5åˆ†ä¸ºä¸€èˆ¬
        self.max_iterations = 10
        self.fast_mode = fast_mode
        self.analysis_max_side = analysis_max_side
        self.temp_dir = Path("temp_enhancement_cache")
        self.temp_dir.mkdir(exist_ok=True)

    def enhance_to_excellent(self, image_path: str, output_dir: str,
                             target_improvement: float = 5.0, max_iterations: int = 10) -> Dict:
        """
        å¢å¼ºå›¾ç‰‡è´¨é‡ï¼Œç›®æ ‡ä¸ºæå‡æŒ‡å®šåˆ†æ•°
        
        å‚æ•°:
            image_path: è¾“å…¥å›¾ç‰‡è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            target_improvement: ç›®æ ‡æå‡åˆ†æ•°ï¼ˆé»˜è®¤5åˆ†ï¼‰
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
        """
        input_path = Path(image_path)
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        if not input_path.exists():
            raise FileNotFoundError(f"è¾“å…¥å›¾ç‰‡ä¸å­˜åœ¨: {image_path}")

        img = cv2.imread(str(input_path))
        if img is None:
            pil_img = Image.open(input_path)
            img = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)

        # è·å–åˆå§‹åˆ†æ•°
        temp_path = self._write_temp_image(img, "initial.jpg")
        initial_analysis = self.analyzer.analyze_single_image(str(temp_path))
        if temp_path.exists():
            temp_path.unlink()
        
        initial_scores = [initial_analysis[dim] for dim in self.analyzer.dimensions]
        initial_score = float(np.mean(initial_scores))

        current_img = img.copy()
        iteration_history = []

        for iteration in range(max_iterations):
            temp_path = self._write_temp_image(current_img, f"iter_{iteration}.jpg")
            analysis_result = self.analyzer.analyze_single_image(str(temp_path))
            if temp_path.exists():
                temp_path.unlink()

            scores = [analysis_result[dim] for dim in self.analyzer.dimensions]
            current_score = float(np.mean(scores))
            improvement = current_score - initial_score
            
            iteration_history.append({
                'iteration': iteration + 1,
                'score': current_score,
                'improvement': improvement,
                'dimension_scores': analysis_result.copy()
            })

            # åˆ¤æ–­æå‡ç­‰çº§
            if improvement >= self.excellent_threshold:
                quality_level = "ä¼˜ç§€"
                target_achieved = True
            elif improvement >= self.good_threshold:
                quality_level = "è‰¯å¥½"
                target_achieved = True
            elif improvement >= self.fair_threshold:
                quality_level = "ä¸€èˆ¬"
                target_achieved = False
            else:
                quality_level = "è¾ƒå·®"
                target_achieved = False

            # å¦‚æœè¾¾åˆ°ç›®æ ‡æå‡å¹…åº¦ï¼Œæå‰ç»“æŸ
            if improvement >= target_improvement:
                final_path = output_path / f"enhanced_final_{input_path.stem}.jpg"
                cv2.imwrite(str(final_path), current_img, [cv2.IMWRITE_JPEG_QUALITY, 95])
                return {
                    'success': True,
                    'target_achieved': True,
                    'initial_score': initial_score,
                    'final_score': current_score,
                    'improvement': improvement,
                    'quality_level': quality_level,
                    'iterations': iteration + 1,
                    'final_image_path': str(final_path),
                    'enhancement_history': iteration_history
                }

            strategies = self._select_enhancement_strategy(analysis_result)
            current_img = self._apply_enhancements(current_img, strategies)

        # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
        final_path = output_path / f"enhanced_final_{input_path.stem}.jpg"
        cv2.imwrite(str(final_path), current_img, [cv2.IMWRITE_JPEG_QUALITY, 95])
        final_improvement = iteration_history[-1]['improvement']
        
        if final_improvement >= self.excellent_threshold:
            quality_level = "ä¼˜ç§€"
        elif final_improvement >= self.good_threshold:
            quality_level = "è‰¯å¥½"
        elif final_improvement >= self.fair_threshold:
            quality_level = "ä¸€èˆ¬"
        else:
            quality_level = "è¾ƒå·®"
        
        return {
            'success': True,
            'target_achieved': final_improvement >= target_improvement,
            'initial_score': initial_score,
            'final_score': iteration_history[-1]['score'],
            'improvement': final_improvement,
            'quality_level': quality_level,
            'iterations': max_iterations,
            'final_image_path': str(final_path),
            'enhancement_history': iteration_history
        }

    def enhance_batch_to_excellent(self, image_paths: List[str], output_dir: str,
                                   target_improvement: float = 5.0, max_iterations: int = 10) -> Dict:
        """
        æ‰¹é‡å¢å¼ºå›¾ç‰‡è´¨é‡
        
        å‚æ•°:
            image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            target_improvement: ç›®æ ‡æå‡åˆ†æ•°
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        batch_results = []
        for img_path in image_paths:
            try:
                img_output_dir = output_path / Path(img_path).stem
                result = self.enhance_to_excellent(img_path, str(img_output_dir),
                                                   target_improvement, max_iterations)
                result['original_path'] = img_path
                batch_results.append(result)
            except Exception as e:
                batch_results.append({'success': False, 'original_path': img_path, 'error': str(e)})

        successful = [r for r in batch_results if r.get('success', False)]
        achieved = [r for r in successful if r.get('target_achieved', False)]
        
        # ç»Ÿè®¡æå‡ç­‰çº§
        excellent_count = sum(1 for r in successful if r.get('improvement', 0) >= self.excellent_threshold)
        good_count = sum(1 for r in successful if self.good_threshold <= r.get('improvement', 0) < self.excellent_threshold)
        fair_count = sum(1 for r in successful if self.fair_threshold <= r.get('improvement', 0) < self.good_threshold)
        poor_count = sum(1 for r in successful if r.get('improvement', 0) < self.fair_threshold)
        
        # è®¡ç®—å¹³å‡æå‡å¹…åº¦
        avg_improvement = np.mean([r.get('improvement', 0) for r in successful]) if successful else 0.0
        
        return {
            'total_images': len(image_paths),
            'successful': len(successful),
            'target_achieved': len(achieved),
            'excellent_count': excellent_count,
            'good_count': good_count,
            'fair_count': fair_count,
            'poor_count': poor_count,
            'average_improvement': avg_improvement,
            'results': batch_results,
            'success_rate': len(successful) / len(image_paths) * 100 if image_paths else 0,
            'achievement_rate': len(achieved) / len(successful) * 100 if successful else 0
        }

    def _bgr_to_tensor(self, img: np.ndarray) -> torch.Tensor:
        tensor = torch.from_numpy(img[:, :, ::-1].copy()).float() / 255.0
        tensor = tensor.permute(2, 0, 1).unsqueeze(0).to(self.device)
        return tensor

    def _tensor_to_bgr(self, tensor: torch.Tensor) -> np.ndarray:
        tensor = tensor.squeeze(0).clamp(0, 1)
        img = (tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
        return img[:, :, ::-1]

    def _write_temp_image(self, img: np.ndarray, filename: str) -> Path:
        temp_path = self.temp_dir / filename
        processed = img
        if self.fast_mode and self.analysis_max_side:
            h, w = img.shape[:2]
            max_side = max(h, w)
            if max_side > self.analysis_max_side:
                scale = self.analysis_max_side / max_side
                new_size = (int(w * scale), int(h * scale))
                processed = cv2.resize(img, new_size, interpolation=cv2.INTER_AREA)
        quality = 85 if self.fast_mode else 95
        cv2.imwrite(str(temp_path), processed, [cv2.IMWRITE_JPEG_QUALITY, quality])
        return temp_path

    def _select_enhancement_strategy(self, scores: Dict) -> List[str]:
        th = 50.0  # é™ä½é˜ˆå€¼ï¼Œå› ä¸ºVisDroneæ•°æ®é›†åˆ†æ•°æœ¬èº«è¾ƒä½
        strategies = []
        if scores.get('å›¾ç‰‡æ•°æ®é‡', 100) < th:
            strategies.append('super_resolution')
        if scores.get('æ‹æ‘„å…‰ç…§è´¨é‡', 100) < th:
            strategies.append('lighting_correction')
            strategies.append('contrast_enhancement')
        if scores.get('ç›®æ ‡å°ºå¯¸', 100) < th:
            strategies.append('sharpen')
            strategies.append('edge_enhancement')
        if scores.get('ç›®æ ‡å®Œæ•´æ€§', 100) < th:
            strategies.append('denoise')
            strategies.append('sharpen')
        if scores.get('æ•°æ®å‡è¡¡åº¦', 100) < th:
            strategies.append('color_enhancement')
        if scores.get('äº§å“ä¸°å¯Œåº¦', 100) < th:
            strategies.append('contrast_enhancement')
            strategies.append('sharpen')
        if scores.get('ç›®æ ‡å¯†é›†åº¦', 100) < th:
            strategies.append('overall_enhancement')
        if scores.get('åœºæ™¯å¤æ‚åº¦', 100) < th:
            strategies.append('texture_enhancement')
            strategies.append('sharpen')
        if not strategies:
            strategies = ['overall_enhancement', 'sharpen', 'contrast_enhancement']
        return strategies

    def _apply_enhancements(self, img: np.ndarray, strategies: List[str]) -> np.ndarray:
        tensor = self._bgr_to_tensor(img)
        for strategy in strategies:
            if strategy == 'super_resolution' and self.fast_mode:
                continue
            if strategy == 'super_resolution':
                tensor = self._super_resolution(tensor)
            elif strategy == 'lighting_correction':
                tensor = self._lighting_correction(tensor)
            elif strategy == 'contrast_enhancement':
                tensor = self._contrast_enhancement(tensor)
            elif strategy == 'sharpen':
                tensor = self._sharpen(tensor)
            elif strategy == 'edge_enhancement':
                tensor = self._edge_enhancement(tensor)
            elif strategy == 'denoise':
                tensor = self._denoise(tensor)
            elif strategy == 'color_enhancement':
                tensor = self._color_enhancement(tensor)
            elif strategy == 'texture_enhancement':
                tensor = self._texture_enhancement(tensor)
            elif strategy == 'overall_enhancement':
                tensor = self._overall_enhancement(tensor)
        return self._tensor_to_bgr(tensor)

    def _super_resolution(self, tensor):
        up = F.interpolate(tensor, scale_factor=1.2, mode='bilinear', align_corners=False)
        _, _, h, w = up.shape
        target_h, target_w = tensor.shape[2:]
        y0 = (h - target_h) // 2
        x0 = (w - target_w) // 2
        return up[:, :, y0:y0 + target_h, x0:x0 + target_w]

    def _lighting_correction(self, tensor):
        gamma = 0.9 if tensor.mean().item() < 0.5 else 1.1
        return torch.clamp(tensor ** gamma, 0, 1)

    def _contrast_enhancement(self, tensor):
        return torch.clamp(tensor * 1.2 + 0.05, 0, 1)

    def _sharpen(self, tensor):
        kernel = torch.tensor([[0, -1, 0], [-1, 5, -1], [0, -1, 0]],
                              device=self.device, dtype=torch.float32)
        kernel = kernel.view(1, 1, 3, 3).repeat(3, 1, 1, 1)
        return torch.clamp(F.conv2d(tensor, kernel, padding=1, groups=3), 0, 1)

    def _edge_enhancement(self, tensor):
        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]],
                               device=self.device, dtype=torch.float32)
        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]],
                               device=self.device, dtype=torch.float32)
        sobel_x = sobel_x.view(1, 1, 3, 3).repeat(3, 1, 1, 1)
        sobel_y = sobel_y.view(1, 1, 3, 3).repeat(3, 1, 1, 1)
        grad = torch.abs(F.conv2d(tensor, sobel_x, padding=1, groups=3)) + \
               torch.abs(F.conv2d(tensor, sobel_y, padding=1, groups=3))
        return torch.clamp(tensor + 0.3 * grad, 0, 1)

    def _denoise(self, tensor):
        kernel = torch.ones((1, 1, 3, 3), device=self.device) / 9.0
        kernel = kernel.repeat(3, 1, 1, 1)
        return torch.clamp(F.conv2d(tensor, kernel, padding=1, groups=3), 0, 1)

    def _color_enhancement(self, tensor):
        mean = tensor.mean(dim=(2, 3), keepdim=True)
        return torch.clamp((tensor - mean) * 1.3 + mean, 0, 1)

    def _texture_enhancement(self, tensor):
        blur = F.avg_pool2d(tensor, kernel_size=3, stride=1, padding=1)
        high_freq = tensor - blur
        return torch.clamp(tensor + 0.5 * high_freq, 0, 1)

    def _overall_enhancement(self, tensor):
        tensor = self._lighting_correction(tensor)
        tensor = self._contrast_enhancement(tensor)
        tensor = self._sharpen(tensor)
        return tensor


if __name__ == "__main__":
    trainer = MaterialEnhancementTrainer()
    result = trainer.enhance_to_excellent(
        "test_image.jpg",
        "enhanced_output",
        target_improvement=5.0,
        max_iterations=10
    )
    print(f"å¢å¼ºå®Œæˆ: {result}")
"@ | Set-Content "agents\material_enhancement_trainer.py" -Encoding UTF8

Write-Host "âœ… æ­¥éª¤2å®Œæˆï¼šå·²æ›´æ–°å¢å¼ºè®­ç»ƒå™¨ï¼ˆæ”¹ä¸ºåŸºäºæå‡å¹…åº¦ï¼‰" -ForegroundColor Green

# ============================================
# æ­¥éª¤3: æ›´æ–°Streamlitç•Œé¢ï¼ˆæ˜¾ç¤ºæå‡å¹…åº¦ï¼‰
# ============================================
Write-Host "`næ­¥éª¤3: æ›´æ–°Streamlitç•Œé¢..." -ForegroundColor Yellow

# è¯»å–ç°æœ‰æ–‡ä»¶
$appContent = Get-Content "app\web\material_generator_app.py" -Raw -Encoding UTF8

# æ›¿æ¢å¢å¼ºè®­ç»ƒè®¾ç½®éƒ¨åˆ†ï¼ˆç¬¬55è¡Œï¼‰
$appContent = $appContent -replace '        target_score = st\.slider\("ç›®æ ‡è´¨é‡åˆ†æ•°", 80, 100, 90, 1\)', '        target_improvement = st.slider("ç›®æ ‡æå‡åˆ†æ•°", 3, 10, 5, 1)'

# æ›¿æ¢åˆ¤æ–­æ˜¯å¦éœ€è¦å¢å¼ºè®­ç»ƒçš„éƒ¨åˆ†ï¼ˆç¬¬230è¡Œï¼‰
$appContent = $appContent -replace '        needs_enhancement = overall_quality < 60\.0', '        needs_enhancement = overall_quality < 50.0  # VisDroneæ•°æ®é›†æ ‡å‡†é™ä½'

# æ›¿æ¢å¢å¼ºè®­ç»ƒè°ƒç”¨éƒ¨åˆ†ï¼ˆç¬¬297è¡Œï¼‰
$appContent = $appContent -replace '                                target_score=target_score,', '                                target_improvement=target_improvement,'

# æ›¿æ¢æ˜¾ç¤ºå¢å¼ºç»“æœçš„éƒ¨åˆ†ï¼ˆç¬¬307è¡Œï¼‰
$oldInfo = '                            st.info(f"ğŸ“Š æˆåŠŸç‡: {enhancement_result[''success_rate'']:.2f}% | è¾¾æ ‡ç‡: {enhancement_result[''achievement_rate'']:.2f}%")'
$newInfo = @'
                            st.info(f"ğŸ“Š æˆåŠŸç‡: {enhancement_result['success_rate']:.2f}% | è¾¾æ ‡ç‡: {enhancement_result['achievement_rate']:.2f}%")
                            st.info(f"ğŸ“ˆ å¹³å‡æå‡å¹…åº¦: {enhancement_result.get('average_improvement', 0):.2f}åˆ†")
                            st.info(f"â­ ä¼˜ç§€({enhancement_result.get('excellent_count', 0)}) | è‰¯å¥½({enhancement_result.get('good_count', 0)}) | ä¸€èˆ¬({enhancement_result.get('fair_count', 0)}) | è¾ƒå·®({enhancement_result.get('poor_count', 0)})")
'@
$appContent = $appContent -replace [regex]::Escape($oldInfo), $newInfo

# æ›¿æ¢å¢å¼ºå†å²è®°å½•æ˜¾ç¤ºéƒ¨åˆ†ï¼ˆç¬¬376è¡Œï¼‰
$oldHistory = "'åˆå§‹åˆ†æ•°': f\"{result.get('enhancement_history', [{}])[0].get('score', 0):.2f}%\" if result.get('enhancement_history') else \"N/A\","
$newHistory = "'åˆå§‹åˆ†æ•°': f\"{result.get('initial_score', 0):.2f}%\","
$appContent = $appContent -replace [regex]::Escape($oldHistory), $newHistory

# æ›¿æ¢æå‡å¹…åº¦æ˜¾ç¤ºï¼ˆç¬¬378è¡Œï¼‰
$oldImprovement = "'æå‡å¹…åº¦': f\"+{result.get('improvement', 0):.2f}%\","
$newImprovement = "'æå‡å¹…åº¦': f\"+{result.get('improvement', 0):.2f}åˆ†\","
$appContent = $appContent -replace [regex]::Escape($oldImprovement), $newImprovement

# æ›¿æ¢æ˜¯å¦è¾¾æ ‡æ˜¾ç¤ºï¼ˆç¬¬380è¡Œï¼‰
$oldTarget = "'æ˜¯å¦è¾¾æ ‡': \"âœ…\" if result.get('target_achieved', False) else \"âŒ\""
$newTarget = "'è´¨é‡ç­‰çº§': result.get('quality_level', 'N/A'),"
$appContent = $appContent -replace [regex]::Escape($oldTarget), $newTarget

# æ›¿æ¢å›¾è¡¨Yè½´æ ‡ç­¾ï¼ˆç¬¬398è¡Œï¼‰
$appContent = $appContent -replace '                    yaxis_title="æå‡å¹…åº¦ \(%\)",', '                    yaxis_title="æå‡å¹…åº¦ (åˆ†)",'

# æ›¿æ¢å›¾è¡¨æ•°æ®è§£æï¼ˆç¬¬390è¡Œï¼‰
$oldYData = "y=[float(item['æå‡å¹…åº¦'].lstrip('+').rstrip('%')) for item in enhancement_history_data],"
$newYData = "y=[float(item['æå‡å¹…åº¦'].lstrip('+').rstrip('åˆ†')) for item in enhancement_history_data],"
$appContent = $appContent -replace [regex]::Escape($oldYData), $newYData

# ä¿å­˜æ›´æ–°åçš„æ–‡ä»¶
$appContent | Set-Content "app\web\material_generator_app.py" -Encoding UTF8

Write-Host "âœ… æ­¥éª¤3å®Œæˆï¼šå·²æ›´æ–°Streamlitç•Œé¢ï¼ˆæ˜¾ç¤ºæå‡å¹…åº¦ï¼‰" -ForegroundColor Green

Write-Host "`nâœ… æ‰€æœ‰æ›´æ–°å®Œæˆï¼" -ForegroundColor Green
Write-Host "`nä¸»è¦æ”¹è¿›ï¼š" -ForegroundColor Cyan
Write-Host "1. 8ç»´è¯„åˆ†æ ‡å‡†å·²é™ä½ï¼Œé€‚é…VisDroneæ•°æ®é›†" -ForegroundColor White
Write-Host "2. å¢å¼ºè®­ç»ƒæ”¹ä¸ºåŸºäºæå‡å¹…åº¦è¯„ä¼°ï¼ˆä¼˜ç§€â‰¥8åˆ†ï¼Œè‰¯å¥½â‰¥5åˆ†ï¼Œä¸€èˆ¬â‰¥3åˆ†ï¼‰" -ForegroundColor White
Write-Host "3. UIç•Œé¢å·²æ›´æ–°ï¼Œæ˜¾ç¤ºæå‡å¹…åº¦å’Œè´¨é‡ç­‰çº§" -ForegroundColor White
Write-Host "`nç°åœ¨å¯ä»¥é‡æ–°è¿è¡ŒStreamlitåº”ç”¨æµ‹è¯•æ–°æ ‡å‡†ï¼" -ForegroundColor Yellow

